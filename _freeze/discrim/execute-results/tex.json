{
  "hash": "f95e5cddb89d67038fa007387f0c29a0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Discriminant Analysis\"\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n## Discriminant analysis\n\n-   ANOVA and MANOVA: predict a (counted/measured) response from group\n    membership.\n\n-   Discriminant analysis: predict group membership based on\n    counted/measured variables.\n\n-   Covers same ground as logistic regression (and its variations), but\n    emphasis on classifying observed data into correct groups.\n\n## ... continued\n\n-   Does so by searching for linear combination of original variables\n    that best separates data into groups (canonical variables).\n\n-   Assumption here that groups are known (for data we have). If trying\n    to \"best separate\" data into unknown groups, see *cluster analysis*.\n\n## Packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS, exclude = \"select\")\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(ggbiplot) # this loads plyr (different from dplyr)\nlibrary(MVTests) # for Box M test\nlibrary(conflicted)\nconflict_prefer(\"arrange\", \"dplyr\")\nconflict_prefer(\"summarize\", \"dplyr\")\nconflict_prefer(\"select\", \"dplyr\")\nconflict_prefer(\"filter\", \"dplyr\")\nconflict_prefer(\"mutate\", \"dplyr\")\nconflicts_prefer(dplyr::count) \n```\n:::\n\n\n\n\n-   `ggrepel` allows labelling points on a plot so they don't overwrite\n    each other.\n-   `ggbiplot` uses `plyr` rather than `dplyr`, which has functions by\n    similar names.\n\n## About `select`\n\n-   Both `dplyr` (in `tidyverse`) and `MASS` have a function called\n    `select`, and *they do different things*.\n\n-   How do you know which `select` is going to get called?\n\n-   With `library`: one loaded *last* visible, others not.\n\n-   Thus we can access the `select` in `dplyr` but not the one in\n    `MASS`. \n    \n-   Better: load `conflicted` package. Any time you load two packages\n    containing functions with same name, get error, choose between them.\n\n## Example 1: seed yields and weights\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/manova1.txt\"\nhilo <- read_delim(my_url, \" \")\ng <- ggplot(hilo, aes(x = yield, y = weight,\n  colour = fertilizer)) + geom_point(size = 4)\n```\n:::\n\n\n\n\n::: columns\n::: {.column width=\"40%\"}\nRecall data from MANOVA: needed a multivariate analysis to find\ndifference in seed yield and weight based on whether they were high or\nlow fertilizer.\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/unnamed-chunk-1-1.pdf)\n:::\n:::\n\n\n\n:::\n:::\n\n## Basic discriminant analysis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhilo.1 <- lda(fertilizer ~ yield + weight, data = hilo)\n```\n:::\n\n\n\n\n-   Uses `lda` from package MASS.\n\n-   \"Predicting\" group membership from measured variables.\n\n## Output (in `hilo.1`)\n\n\\small\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(fertilizer ~ yield + weight, data = hilo)\n\nPrior probabilities of groups:\nhigh  low \n 0.5  0.5 \n\nGroup means:\n     yield weight\nhigh  35.0  13.25\nlow   32.5  12.00\n\nCoefficients of linear discriminants:\n              LD1\nyield  -0.7666761\nweight -1.2513563\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Things to take from output 1/2\n\n-   Group means: high-fertilizer plants have (slightly) higher mean\n    yield and weight than low-fertilizer plants.\n\n-   \"Coefficients of linear discriminants\": \\texttt{LD1,\n    LD2,}\\ldots are scores constructed from observed variables that best\n    separate the groups.\n\n-   For any plant, get LD1 score by taking $-0.76$ times yield plus\n    $-1.25$ times weight, add up, standardize.\n\n## Things to take from output 1/2\n\n-   the LD1 coefficients are like slopes:\n\n    -   if yield higher, LD1 score for a plant lower\n    -   if weight higher, LD1 score for a plant lower\n\n-   High-fertilizer plants have higher yield and weight, thus low\n    (negative) LD1 score. Low-fertilizer plants have low yield and\n    weight, thus high (positive) LD1 score.\n\n-   One LD1 score for each observation. Plot with actual groups.\n\n## How many linear discriminants?\n\n-   Smaller of these:\n\n    -   Number of variables\n\n    -   Number of groups *minus 1*\n\n-   Seed yield and weight: 2 variables, 2 groups, $\\min(2,2-1)=1$.\n\n## Getting LD scores\n\nFeed output from LDA into `predict`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(hilo.1)\nhilo.2 <- cbind(hilo, p)\n```\n:::\n\n\n\n\n## the LD scores\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhilo.2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  fertilizer yield weight class posterior.high posterior.low        LD1\n1        low    34     10   low   2.108619e-05  9.999789e-01  3.0931414\n2        low    29     14   low   1.245320e-03  9.987547e-01  1.9210963\n3        low    35     11   low   2.315016e-02  9.768498e-01  1.0751090\n4        low    32     13   low   4.579036e-02  9.542096e-01  0.8724245\n5       high    33     14  high   9.817958e-01  1.820422e-02 -1.1456079\n6       high    38     12  high   9.998195e-01  1.804941e-04 -2.4762756\n7       high    34     13  high   9.089278e-01  9.107216e-02 -0.6609276\n8       high    35     14  high   9.999109e-01  8.914534e-05 -2.6789600\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## LD1 scores in order\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhilo.2 %>% select(fertilizer, yield, weight, LD1) %>% \n  arrange(desc(LD1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  fertilizer yield weight        LD1\n1        low    34     10  3.0931414\n2        low    29     14  1.9210963\n3        low    35     11  1.0751090\n4        low    32     13  0.8724245\n7       high    34     13 -0.6609276\n5       high    33     14 -1.1456079\n6       high    38     12 -2.4762756\n8       high    35     14 -2.6789600\n```\n\n\n:::\n:::\n\n\n\n\n## LD1 scores and fertilizer\n\nMost positive LD1 score is most obviously low fertilizer, most negative\nis most obviously high.\n\nHigh fertilizer have yield and weight high, negative LD1 scores.\n\n## Plotting LD1 scores\n\nWith one LD score, plot against (true) groups, eg. boxplot:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(hilo.2, aes(x = fertilizer, y = LD1)) + geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/bDiscrim-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## What else is in `hilo.2`?\n\n-   `class`: predicted fertilizer level (based on values of `yield` and\n    `weight`).\n\n-   `posterior`: predicted probability of being low or high fertilizer\n    given `yield` and `weight`.\n\n-   `LD1`: scores for (each) linear discriminant (here is only LD1) on\n    each observation.\n\n## Predictions and predicted groups\n\n\\ldots based on `yield` and `weight`:\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhilo.2 %>% select(yield, weight, fertilizer, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  yield weight fertilizer class\n1    34     10        low   low\n2    29     14        low   low\n3    35     11        low   low\n4    32     13        low   low\n5    33     14       high  high\n6    38     12       high  high\n7    34     13       high  high\n8    35     14       high  high\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Count up correct and incorrect classification\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(hilo.2, table(obs = fertilizer, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      pred\nobs    high low\n  high    4   0\n  low     0   4\n```\n\n\n:::\n:::\n\n\n\n\n-   Each predicted fertilizer level is exactly same as observed one\n    (perfect prediction).\n\n-   Table shows no errors: all values on top-left to bottom-right\n    diagonal.\n\n## Posterior probabilities\n\n\\footnotesize\n\nshow how clear-cut the classification decisions were:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhilo.2 %>% \n  mutate(across(starts_with(\"posterior\"), \\(p) round(p, 4))) %>% \n  select(-LD1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  fertilizer yield weight class posterior.high posterior.low\n1        low    34     10   low         0.0000        1.0000\n2        low    29     14   low         0.0012        0.9988\n3        low    35     11   low         0.0232        0.9768\n4        low    32     13   low         0.0458        0.9542\n5       high    33     14  high         0.9818        0.0182\n6       high    38     12  high         0.9998        0.0002\n7       high    34     13  high         0.9089        0.0911\n8       high    35     14  high         0.9999        0.0001\n```\n\n\n:::\n:::\n\n\n\n\n## Comments\n\nOnly obs.Â 7 has any doubt: `yield` low for a high-fertilizer, but high\n`weight` makes up for it.\n\n\n## Example 2: the peanuts\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/peanuts.txt\"\npeanuts <- read_delim(my_url, \" \")\npeanuts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 6\n     obs location variety     y   smk     w\n   <dbl>    <dbl>   <dbl> <dbl> <dbl> <dbl>\n 1     1        1       5  195.  153.  51.4\n 2     2        1       5  194.  168.  53.7\n 3     3        2       5  190.  140.  55.5\n 4     4        2       5  180.  121.  44.4\n 5     5        1       6  203   157.  49.8\n 6     6        1       6  196.  166   45.8\n 7     7        2       6  203.  166.  60.4\n 8     8        2       6  198.  162.  54.1\n 9     9        1       8  194.  164.  57.8\n10    10        1       8  187   165.  58.6\n11    11        2       8  202.  167.  65  \n12    12        2       8  200   174.  67.2\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comment\n\n-   Recall: `location` and `variety` both significant in MANOVA. \n-   Make combo of them:\n\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts %>%\n   unite(combo, c(variety, location)) -> peanuts.combo\npeanuts.combo\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 5\n     obs combo     y   smk     w\n   <dbl> <chr> <dbl> <dbl> <dbl>\n 1     1 5_1    195.  153.  51.4\n 2     2 5_1    194.  168.  53.7\n 3     3 5_2    190.  140.  55.5\n 4     4 5_2    180.  121.  44.4\n 5     5 6_1    203   157.  49.8\n 6     6 6_1    196.  166   45.8\n 7     7 6_2    203.  166.  60.4\n 8     8 6_2    198.  162.  54.1\n 9     9 8_1    194.  164.  57.8\n10    10 8_1    187   165.  58.6\n11    11 8_2    202.  167.  65  \n12    12 8_2    200   174.  67.2\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Discriminant analysis\n\n\\tiny\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# peanuts.1 <- lda(str_c(location, variety, sep = \"_\") ~ y + smk + w, data = peanuts)\npeanuts.1 <- lda(combo ~ y + smk + w, data = peanuts.combo)\npeanuts.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(combo ~ y + smk + w, data = peanuts.combo)\n\nPrior probabilities of groups:\n      5_1       5_2       6_1       6_2       8_1       8_2 \n0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 \n\nGroup means:\n         y    smk     w\n5_1 194.80 160.40 52.55\n5_2 185.05 130.30 49.95\n6_1 199.45 161.40 47.80\n6_2 200.15 163.95 57.25\n8_1 190.25 164.80 58.20\n8_2 200.75 170.30 66.10\n\nCoefficients of linear discriminants:\n           LD1         LD2         LD3\ny    0.4027356  0.02967881  0.18839237\nsmk  0.1727459 -0.06794271 -0.09386294\nw   -0.5792456 -0.16300221  0.07341123\n\nProportion of trace:\n   LD1    LD2    LD3 \n0.8424 0.1317 0.0258 \n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comments\n\n-   Now 3 LDs (3 variables, 6 groups, $\\min(3,6-1)=3$).\n\n-   Relationship of LDs to original variables. Look for coeffs far from\n    zero:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts.1$scaling\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           LD1         LD2         LD3\ny    0.4027356  0.02967881  0.18839237\nsmk  0.1727459 -0.06794271 -0.09386294\nw   -0.5792456 -0.16300221  0.07341123\n```\n\n\n:::\n:::\n\n\n\n\n-   high `LD1` mainly high `y` or low `w`.\n\n-   high `LD2` mainly low `w`.\n\n-   Proportion of trace values show relative importance of LDs: `LD1`\n    much more important than `LD2`; `LD3` worthless.\n\n## The predictions, badly\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(peanuts.1)\npeanuts.2 <- cbind(peanuts.combo, p)\npeanuts.2 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   obs combo     y   smk    w class posterior.5_1 posterior.5_2 posterior.6_1\n1    1   5_1 195.3 153.1 51.4   5_1  6.862288e-01  1.825787e-12  1.626712e-06\n2    2   5_1 194.3 167.7 53.7   5_1  7.269338e-01  7.555850e-17  1.265614e-05\n3    3   5_2 189.7 139.5 55.5   5_2  1.624097e-12  9.996353e-01  1.501005e-32\n4    4   5_2 180.4 121.1 44.4   5_2  1.702156e-16  1.000000e+00  1.070250e-36\n5    5   6_1 203.0 156.8 49.8   6_1  4.262552e-05  1.500083e-31  9.999036e-01\n6    6   6_1 195.9 166.0 45.8   6_1  9.681355e-07  1.071193e-37  9.999989e-01\n7    7   6_2 202.7 166.1 60.4   6_2  1.324922e-01  5.989065e-15  7.932019e-08\n8    8   6_2 197.6 161.8 54.1   5_1  5.286987e-01  2.037992e-16  3.255237e-05\n9    9   8_1 193.5 164.5 57.8   8_1  2.298649e-02  6.924748e-08  5.529930e-14\n10  10   8_1 187.0 165.1 58.6   8_1  1.572134e-08  5.773681e-05  1.026123e-26\n11  11   8_2 201.5 166.8 65.0   8_2  8.160707e-05  6.481495e-09  1.219898e-17\n12  12   8_2 200.0 173.8 67.2   8_2  1.509768e-06  1.557142e-09  3.094904e-21\n   posterior.6_2 posterior.8_1 posterior.8_2     x.LD1       x.LD2       x.LD3\n1   3.137397e-01  1.789618e-05  1.198440e-05  1.417354  1.01233393  0.26467918\n2   2.730344e-01  1.351022e-05  5.606435e-06  2.204444 -0.38421359 -1.12526629\n3   8.539384e-13  3.555819e-04  9.130935e-06 -5.562217  1.10184441  0.78720394\n4   3.502685e-18  1.507494e-08  1.207900e-12 -6.056558  3.88530191 -0.05263163\n5   5.379795e-05  5.404834e-19  3.274483e-17  6.084370  1.25027629  1.25054957\n6   1.176475e-07  1.407963e-21  1.260338e-21  7.131192  1.06649258 -1.24422021\n7   8.655891e-01  2.872010e-05  1.889878e-03  1.430084 -1.11831802  1.09926555\n8   4.712635e-01  1.387366e-06  3.934555e-06  2.282572  0.04938762  0.07958437\n9   1.764211e-02  7.502924e-01  2.090789e-01 -1.045438 -0.85884902 -0.67463274\n10  4.128139e-09  9.937398e-01  6.202449e-03 -4.022969 -1.22292871 -1.89677191\n11  9.872502e-04  2.708552e-02  9.718456e-01 -1.596806 -1.95130266  1.14518230\n12  1.688982e-05  5.873968e-02  9.412419e-01 -2.266028 -2.83002474  0.36705787\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comments \n\n- Hard to read:\n  - The posterior probabilities are in scientific notation\n  - The *names* of the `posterior` columns are rather long; names like `p.5_1` would be better.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts.2 %>% \n  mutate(across(starts_with(\"posterior\"), \n                            \\(p) round(p, 3))) %>% \n  rename_with(\\(p) str_remove(p, \"osterior\"), \n              starts_with(\"posterior\")) -> peanuts.2a\n```\n:::\n\n\n\n\n## Result (slightly better)\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts.2a\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   obs combo     y   smk    w class p.5_1 p.5_2 p.6_1 p.6_2 p.8_1 p.8_2\n1    1   5_1 195.3 153.1 51.4   5_1 0.686     0     0 0.314 0.000 0.000\n2    2   5_1 194.3 167.7 53.7   5_1 0.727     0     0 0.273 0.000 0.000\n3    3   5_2 189.7 139.5 55.5   5_2 0.000     1     0 0.000 0.000 0.000\n4    4   5_2 180.4 121.1 44.4   5_2 0.000     1     0 0.000 0.000 0.000\n5    5   6_1 203.0 156.8 49.8   6_1 0.000     0     1 0.000 0.000 0.000\n6    6   6_1 195.9 166.0 45.8   6_1 0.000     0     1 0.000 0.000 0.000\n7    7   6_2 202.7 166.1 60.4   6_2 0.132     0     0 0.866 0.000 0.002\n8    8   6_2 197.6 161.8 54.1   5_1 0.529     0     0 0.471 0.000 0.000\n9    9   8_1 193.5 164.5 57.8   8_1 0.023     0     0 0.018 0.750 0.209\n10  10   8_1 187.0 165.1 58.6   8_1 0.000     0     0 0.000 0.994 0.006\n11  11   8_2 201.5 166.8 65.0   8_2 0.000     0     0 0.001 0.027 0.972\n12  12   8_2 200.0 173.8 67.2   8_2 0.000     0     0 0.000 0.059 0.941\n       x.LD1       x.LD2       x.LD3\n1   1.417354  1.01233393  0.26467918\n2   2.204444 -0.38421359 -1.12526629\n3  -5.562217  1.10184441  0.78720394\n4  -6.056558  3.88530191 -0.05263163\n5   6.084370  1.25027629  1.25054957\n6   7.131192  1.06649258 -1.24422021\n7   1.430084 -1.11831802  1.09926555\n8   2.282572  0.04938762  0.07958437\n9  -1.045438 -0.85884902 -0.67463274\n10 -4.022969 -1.22292871 -1.89677191\n11 -1.596806 -1.95130266  1.14518230\n12 -2.266028 -2.83002474  0.36705787\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Misclassification\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(peanuts.2, table(obs = combo, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     pred\nobs   5_1 5_2 6_1 6_2 8_1 8_2\n  5_1   2   0   0   0   0   0\n  5_2   0   2   0   0   0   0\n  6_1   0   0   2   0   0   0\n  6_2   1   0   0   1   0   0\n  8_1   0   0   0   0   2   0\n  8_2   0   0   0   0   0   2\n```\n\n\n:::\n:::\n\n\n\n\nActually classified very well. Only one `6_2` classified as a `5_1`,\nrest all correct.\n\n## Posterior probabilities\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts.2a %>% \n  select(combo,  class, starts_with(\"p\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   combo class p.5_1 p.5_2 p.6_1 p.6_2 p.8_1 p.8_2\n1    5_1   5_1 0.686     0     0 0.314 0.000 0.000\n2    5_1   5_1 0.727     0     0 0.273 0.000 0.000\n3    5_2   5_2 0.000     1     0 0.000 0.000 0.000\n4    5_2   5_2 0.000     1     0 0.000 0.000 0.000\n5    6_1   6_1 0.000     0     1 0.000 0.000 0.000\n6    6_1   6_1 0.000     0     1 0.000 0.000 0.000\n7    6_2   6_2 0.132     0     0 0.866 0.000 0.002\n8    6_2   5_1 0.529     0     0 0.471 0.000 0.000\n9    8_1   8_1 0.023     0     0 0.018 0.750 0.209\n10   8_1   8_1 0.000     0     0 0.000 0.994 0.006\n11   8_2   8_2 0.000     0     0 0.001 0.027 0.972\n12   8_2   8_2 0.000     0     0 0.000 0.059 0.941\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comments \n\n- *Some* doubt about which combo each plant belongs in, but not too much.\n- The one misclassified plant (row 8) was a close call.\n\n## Discriminant scores, again\n\n-   How are discriminant scores related to original variables?\n\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts.1$scaling\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           LD1         LD2         LD3\ny    0.4027356  0.02967881  0.18839237\nsmk  0.1727459 -0.06794271 -0.09386294\nw   -0.5792456 -0.16300221  0.07341123\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n-   LD1 positive if `y` large and/or `w` small.\n\n-   LD2 positive if `w` small.\n\n\n## Discriminant scores for data\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts.2 %>% select(y, w, starts_with(\"x\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       y    w     x.LD1       x.LD2       x.LD3\n1  195.3 51.4  1.417354  1.01233393  0.26467918\n2  194.3 53.7  2.204444 -0.38421359 -1.12526629\n3  189.7 55.5 -5.562217  1.10184441  0.78720394\n4  180.4 44.4 -6.056558  3.88530191 -0.05263163\n5  203.0 49.8  6.084370  1.25027629  1.25054957\n6  195.9 45.8  7.131192  1.06649258 -1.24422021\n7  202.7 60.4  1.430084 -1.11831802  1.09926555\n8  197.6 54.1  2.282572  0.04938762  0.07958437\n9  193.5 57.8 -1.045438 -0.85884902 -0.67463274\n10 187.0 58.6 -4.022969 -1.22292871 -1.89677191\n11 201.5 65.0 -1.596806 -1.95130266  1.14518230\n12 200.0 67.2 -2.266028 -2.83002474  0.36705787\n```\n\n\n:::\n:::\n\n\n\n\n-   Obs. 5 and 6 have most positive `LD1`: large `y`, small `w`.\n\n-   Obs. 4 has most positive `LD2`: small `w`.\n\n\\normalsize\n\n## Plot LD1 vs. LD2, labelling by combo\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- ggplot(peanuts.2, aes(x = x.LD1, y = x.LD2, colour = combo, \n                    label = combo)) + geom_point() +\n  geom_text_repel() + guides(colour = \"none\")\ng\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/bDiscrim-24-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\\normalsize\n\n## \"Bi-plot\" from `ggbiplot`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot(peanuts.1, groups = factor(peanuts.combo$combo))\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/bDiscrim-25-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Installing `ggbiplot`\n\n-   `ggbiplot` not on CRAN, so usual `install.packages` will not work.\n\n-   Install package `devtools` first (once):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"devtools\")\n```\n:::\n\n\n\n\n-   Then install `ggbiplot` (once):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(devtools)\ninstall_github(\"vqv/ggbiplot\")\n```\n:::\n\n\n\n\n## Cross-validation\n\n-   So far, have predicted group membership from same data used to form\n    the groups --- dishonest!\n\n-   Better: *cross-validation*: form groups from all observations\n    *except one*, then predict group membership for that left-out\n    observation.\n\n-   No longer cheating!\n\n-   Illustrate with peanuts data again.\n\n## Misclassifications\n\n-   Fitting and prediction all in one go:\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- lda(combo ~ y + smk + w,\n  data = peanuts.combo, CV = TRUE)\npeanuts.3 <- cbind(peanuts.combo, class = p$class, \n                   posterior = p$posterior)\nwith(peanuts.3, table(obs = combo, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     pred\nobs   5_1 5_2 6_1 6_2 8_1 8_2\n  5_1   0   0   0   2   0   0\n  5_2   0   1   0   0   1   0\n  6_1   0   0   2   0   0   0\n  6_2   1   0   0   1   0   0\n  8_1   0   1   0   0   0   1\n  8_2   0   0   0   0   0   2\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n-   Some more misclassification this time.\n\n## Repeat of LD plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/graziani-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Posterior probabilities\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeanuts.3 %>% \n  mutate(across(starts_with(\"posterior\"), \\(p) round(p, 3))) %>% \n  rename_with(\\(p) str_remove(p, \"osterior\"), \n              starts_with(\"posterior\")) %>% \n  select(combo, class, starts_with(\"p.\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   combo class p.5_1 p.5_2 p.6_1 p.6_2 p.8_1 p.8_2\n1    5_1   6_2 0.162  0.00 0.000 0.838 0.000 0.000\n2    5_1   6_2 0.200  0.00 0.000 0.799 0.000 0.000\n3    5_2   8_1 0.000  0.18 0.000 0.000 0.820 0.000\n4    5_2   5_2 0.000  1.00 0.000 0.000 0.000 0.000\n5    6_1   6_1 0.194  0.00 0.669 0.137 0.000 0.000\n6    6_1   6_1 0.000  0.00 1.000 0.000 0.000 0.000\n7    6_2   6_2 0.325  0.00 0.000 0.667 0.001 0.008\n8    6_2   5_1 0.821  0.00 0.000 0.179 0.000 0.000\n9    8_1   8_2 0.000  0.00 0.000 0.000 0.000 1.000\n10   8_1   5_2 0.000  1.00 0.000 0.000 0.000 0.000\n11   8_2   8_2 0.001  0.00 0.000 0.004 0.083 0.913\n12   8_2   8_2 0.000  0.00 0.000 0.000 0.167 0.833\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Why more misclassification?\n\n-   When predicting group membership for one observation, only uses the\n    *other one* in that group.\n\n-   So if two in a pair are far apart, or if two groups overlap, great\n    potential for misclassification.\n\n-   Groups `5_1` and `6_2` overlap.\n\n-   `5_2` closest to `8_1`s looks more like an `8_1` than a `5_2` (other\n    one far away).\n\n-   `8_1`s relatively far apart and close to other things, so one\n    appears to be a `5_2` and the other an `8_2`.\n\n## Example 3: professions and leisure activities\n\n-   15 individuals from three different professions (politicians,\n    administrators and belly dancers) each participate in four different\n    leisure activities: reading, dancing, TV watching and skiing. After\n    each activity they rate it on a 0--10 scale.\n\n-   How can we best use the scores on the activities to predict a\n    person's profession?\n\n-   Or, what combination(s) of scores best separate data into profession\n    groups?\n\n## The data\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/profile.txt\"\nactive <- read_delim(my_url, \" \")\nactive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 x 5\n   job         reading dance    tv   ski\n   <chr>         <dbl> <dbl> <dbl> <dbl>\n 1 bellydancer       7    10     6     5\n 2 bellydancer       8     9     5     7\n 3 bellydancer       5    10     5     8\n 4 bellydancer       6    10     6     8\n 5 bellydancer       7     8     7     9\n 6 politician        4     4     4     4\n 7 politician        6     4     5     3\n 8 politician        5     5     5     6\n 9 politician        6     6     6     7\n10 politician        4     5     6     5\n11 admin             3     1     1     2\n12 admin             5     3     1     5\n13 admin             4     2     2     5\n14 admin             7     1     2     4\n15 admin             6     3     3     3\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Discriminant analysis\n\n\\tiny\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactive.1 <- lda(job ~ reading + dance + tv + ski, data = active)\nactive.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(job ~ reading + dance + tv + ski, data = active)\n\nPrior probabilities of groups:\n      admin bellydancer  politician \n  0.3333333   0.3333333   0.3333333 \n\nGroup means:\n            reading dance  tv ski\nadmin           5.0   2.0 1.8 3.8\nbellydancer     6.6   9.4 5.8 7.4\npolitician      5.0   4.8 5.2 5.0\n\nCoefficients of linear discriminants:\n                LD1        LD2\nreading -0.01297465 -0.4748081\ndance   -0.95212396 -0.4614976\ntv      -0.47417264  1.2446327\nski      0.04153684 -0.2033122\n\nProportion of trace:\n   LD1    LD2 \n0.8917 0.1083 \n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comments\n\n-   Two discriminants, first fair bit more important than second.\n\n-   `LD1` depends (negatively) most on `dance`, a bit on `tv`.\n\n-   `LD2` depends mostly (positively) on `tv`.\n\n## Misclassification\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(active.1)\nactive.2 <- cbind(active, p)\nwith(active.2, table(obs = job, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             pred\nobs           admin bellydancer politician\n  admin           5           0          0\n  bellydancer     0           5          0\n  politician      0           0          5\n```\n\n\n:::\n:::\n\n\n\n\nEveryone correctly classified.\n\n## Plotting LDs\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- ggplot(active.2, aes(x = x.LD1, y = x.LD2, colour = job, label = job)) + \n  geom_point() + geom_text_repel() + guides(colour = \"none\")\ng\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/bDiscrim-32-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Biplot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot(active.1, groups = active$job)\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/bDiscrim-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments on plot\n\n-   Groups well separated: bellydancers top left, administrators top\n    right, politicians lower middle.\n\n-   Bellydancers most negative on `LD1`: like dancing most.\n\n-   Administrators most positive on `LD1`: like dancing least.\n\n-   Politicians most negative on `LD2`: like TV-watching most.\n\n## Plotting individual `persons`\n\nMake `label` be identifier of person. Now need legend:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactive.2 %>% mutate(person = row_number()) %>% \n  ggplot(aes(x = x.LD1, y = x.LD2,  colour = job, \n               label = person)) + \n  geom_point() + geom_text_repel()\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/bDiscrim-34-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Posterior probabilities\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactive.2 %>% mutate(across(starts_with(\"posterior\"), \\(p) round(p, 3))) %>% \n  rename_with(\\(p) str_remove(p, \"osterior\"), \n              starts_with(\"posterior\")) %>% \n  select(job, class, starts_with(\"p.\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           job       class p.admin p.bellydancer p.politician\n1  bellydancer bellydancer   0.000         1.000        0.000\n2  bellydancer bellydancer   0.000         1.000        0.000\n3  bellydancer bellydancer   0.000         1.000        0.000\n4  bellydancer bellydancer   0.000         1.000        0.000\n5  bellydancer bellydancer   0.000         0.997        0.003\n6   politician  politician   0.003         0.000        0.997\n7   politician  politician   0.000         0.000        1.000\n8   politician  politician   0.000         0.000        1.000\n9   politician  politician   0.000         0.002        0.998\n10  politician  politician   0.000         0.000        1.000\n11       admin       admin   1.000         0.000        0.000\n12       admin       admin   1.000         0.000        0.000\n13       admin       admin   1.000         0.000        0.000\n14       admin       admin   1.000         0.000        0.000\n15       admin       admin   0.982         0.000        0.018\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\nNot much doubt.\n\n## Cross-validating the jobs-activities data\n\nRecall: no need for `predict`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- lda(job ~ reading + dance + tv + ski, data = active, CV = TRUE)\nactive.3 <- cbind(active, class = p$class, posterior = p$posterior)\nwith(active.3, table(obs = job, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             pred\nobs           admin bellydancer politician\n  admin           5           0          0\n  bellydancer     0           4          1\n  politician      0           0          5\n```\n\n\n:::\n:::\n\n\n\n\nThis time one of the bellydancers was classified as a politician.\n\n## and look at the posterior probabilities\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactive.3 %>% \n  mutate(across(starts_with(\"posterior\"), \\(p) round(p, 3))) %>% \n  rename_with(\\(p) str_remove(p, \"osterior\"), \n              starts_with(\"posterior\")) %>% \n  select(job, class, starts_with(\"p.\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           job       class p.admin p.bellydancer p.politician\n1  bellydancer bellydancer   0.000         1.000        0.000\n2  bellydancer bellydancer   0.000         1.000        0.000\n3  bellydancer bellydancer   0.000         1.000        0.000\n4  bellydancer bellydancer   0.000         1.000        0.000\n5  bellydancer  politician   0.000         0.001        0.999\n6   politician  politician   0.006         0.000        0.994\n7   politician  politician   0.001         0.000        0.999\n8   politician  politician   0.000         0.000        1.000\n9   politician  politician   0.000         0.009        0.991\n10  politician  politician   0.000         0.000        1.000\n11       admin       admin   1.000         0.000        0.000\n12       admin       admin   1.000         0.000        0.000\n13       admin       admin   1.000         0.000        0.000\n14       admin       admin   1.000         0.000        0.000\n15       admin       admin   0.819         0.000        0.181\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comments\n\n-   Bellydancer was \"definitely\" a politician!\n\n-   One of the administrators might have been a politician too.\n\n## Why did things get misclassified?\n\n::: columns\n::: {.column width=\"40%\"}\nGo back to plot of discriminant scores:\n\n-   one bellydancer much closer to the politicians,\n\n-   one administrator a bit closer to the politicians.\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\n\n:::\n:::\n\n## Example 4: remote-sensing data\n\n-   View 25 crops from air, measure 4 variables `x1-x4`.\n\n-   Go back and record what each crop was.\n\n-   Can we use the 4 variables to distinguish crops?\n\n## The data\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/remote-sensing.txt\"\ncrops <- read_table(my_url)\ncrops\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 25 x 6\n   crop        x1    x2    x3    x4 cr   \n   <chr>    <dbl> <dbl> <dbl> <dbl> <chr>\n 1 Corn        16    27    31    33 r    \n 2 Corn        15    23    30    30 r    \n 3 Corn        16    27    27    26 r    \n 4 Corn        18    20    25    23 r    \n 5 Corn        15    15    31    32 r    \n 6 Corn        15    32    32    15 r    \n 7 Corn        12    15    16    73 r    \n 8 Soybeans    20    23    23    25 y    \n 9 Soybeans    24    24    25    32 y    \n10 Soybeans    21    25    23    24 y    \n# i 15 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Discriminant analysis\n\n\\tiny\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrops.1 <- lda(crop ~ x1 + x2 + x3 + x4, data = crops)\ncrops.1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(crop ~ x1 + x2 + x3 + x4, data = crops)\n\nPrior probabilities of groups:\n      Corn     Cotton   Soybeans Sugarbeets \n      0.28       0.24       0.24       0.24 \n\nGroup means:\n                 x1       x2       x3       x4\nCorn       15.28571 22.71429 27.42857 33.14286\nCotton     34.50000 32.66667 35.00000 39.16667\nSoybeans   21.00000 27.00000 23.50000 29.66667\nSugarbeets 31.00000 32.16667 20.00000 40.50000\n\nCoefficients of linear discriminants:\n           LD1          LD2           LD3\nx1  0.14077479  0.007780184 -0.0312610362\nx2  0.03006972  0.007318386  0.0085401510\nx3 -0.06363974 -0.099520895 -0.0005309869\nx4 -0.00677414 -0.035612707  0.0577718649\n\nProportion of trace:\n   LD1    LD2    LD3 \n0.8044 0.1832 0.0124 \n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Assessing\n\n-   3 LDs (four variables, four groups).\n\n-   1st two important.\n\n-   `LD1` mostly `x1` (plus)\n\n-   `LD2` `x3` (minus)\n\n## Predictions\n\n-   Thus:\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- predict(crops.1)\ncrops.2 <- cbind(crops, p)\nwith(crops.2, table(obs = crop, pred = class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            pred\nobs          Corn Cotton Soybeans Sugarbeets\n  Corn          6      0        1          0\n  Cotton        0      4        2          0\n  Soybeans      2      0        3          1\n  Sugarbeets    0      0        3          3\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n-   Not very good, eg. only half the Soybeans and Sugarbeets classified\n    correctly.\n\n## Plotting the LDs\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(crops.2, aes(x = x.LD1, y = x.LD2, colour = crop)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/piacentini-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nCorn (red) mostly left, cotton (green) sort of right, soybeans and\nsugarbeets (blue and purple) mixed up.\n\n## Biplot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggbiplot(crops.1, groups = crops$crop)\n```\n\n::: {.cell-output-display}\n![](discrim_files/figure-beamer/bDiscrim-45-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   Corn low on LD1 (left), hence low on `x1`\n\n-   Cotton tends to be high on LD1 (high `x1`)\n\n-   one cotton very low on LD2 (high `x3`?)\n\n-   Rather mixed up.\n\n## Posterior probs (some)\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrops.2 %>% mutate(across(starts_with(\"posterior\"), \\(p) round(p, 3))) %>% \n  rename_with(\\(p) str_remove(p, \"osterior\"), \n              starts_with(\"posterior\")) %>% \n  filter(crop != class) %>% \n  select(crop, class, starts_with(\"p.\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         crop      class p.Corn p.Cotton p.Soybeans p.Sugarbeets\n4        Corn   Soybeans  0.443    0.034      0.494        0.029\n11   Soybeans Sugarbeets  0.010    0.107      0.299        0.584\n12   Soybeans       Corn  0.684    0.009      0.296        0.011\n13   Soybeans       Corn  0.467    0.199      0.287        0.047\n15     Cotton   Soybeans  0.056    0.241      0.379        0.324\n17     Cotton   Soybeans  0.066    0.138      0.489        0.306\n20 Sugarbeets   Soybeans  0.381    0.146      0.395        0.078\n21 Sugarbeets   Soybeans  0.106    0.144      0.518        0.232\n24 Sugarbeets   Soybeans  0.088    0.207      0.489        0.216\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comments\n\n-   These were the misclassified ones, but the posterior probability of\n    being correct was not usually too low.\n\n-   The correctly-classified ones are not very clear-cut either.\n\n## MANOVA\n\nBegan discriminant analysis as a followup to MANOVA. Do our variables\nsignificantly separate the crops?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- with(crops, cbind(x1, x2, x3, x4))\ncrops.manova <- manova(response ~ crop, data = crops)\nsummary(crops.manova)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Df Pillai approx F num Df den Df  Pr(>F)  \ncrop       3 0.9113   2.1815     12     60 0.02416 *\nResiduals 21                                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n## Box's M test\n\nWe should also run Box's M test to check for equal variance of each\nvariable across crops:\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(BoxM(response, crops$crop))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Box's M Test \n\nChi-Squared Value = 69.42634 , df = 30  and p-value: 5.79e-05 \n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n-   The P-value for the M test is smaller even than our guideline of\n    0.001. So we should not take the MANOVA seriously.\n\n-   *Apparently* at least one of the crops differs (in means) from the\n    others. So it is worth doing this analysis.\n\n-   We did this the wrong way around, though!\n\n## The right way around\n\n-   *First*, do a MANOVA to see whether any of the groups differ\n    significantly on any of the variables.\n\n-   Check that the MANOVA is believable by using Box's M test.\n\n-   *If the MANOVA is significant*, do a discriminant analysis in the\n    hopes of understanding how the groups are different.\n\n-   For remote-sensing data (without Clover):\n\n    -   LD1 a fair bit more important than LD2 (definitely ignore LD3).\n\n    -   LD1 depends mostly on `x1`, on which Cotton was high and Corn\n        was low.\n\n-   Discriminant analysis in MANOVA plays the same kind of role that\n    Tukey does in ANOVA.\n",
    "supporting": [
      "discrim_files/figure-beamer"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}