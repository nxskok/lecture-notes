{
  "hash": "995ef842807af015af77f53b0f7c01f9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Alternative tests\"\n---\n\n\n\n\n## When there isn't sufficient normality\n\n- When your samples are not normal enough, cannot use $t$ procedures\n- Sometimes transforming the data (eg taking logs of all the values) will help\n- Or, use test with no assumptions about normality:\n  - for one sample, use *sign test* for median\n  - for two samples, use *Mood's median test*\n  - for matched pairs, use *sign test* on differences.\n  \n  \n##  One-sample: the IRS data\n\n-   The IRS (\"Internal Revenue Service\") is the US authority that deals\n    with taxes (like Revenue Canada).\n-   One of their forms is supposed to take no more than 160 minutes to\n    complete. A citizen's organization claims that it takes people\n    longer than that on average.\n-   Sample of 30 people; time to complete form recorded.\n-   Read in data, and do $t$-test of $H_0 : \\mu = 160$ vs.\n    $H_a : \\mu > 160$.\n-   Only one column, so pretend it is\n    delimited by something.\n\n## Packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(smmr) \n```\n:::\n\n\n\n\n- installation instructions for `smmr` later\n\n## Read in data\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/irs.txt\"\nirs <- read_csv(my_url)\nirs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 x 1\n    Time\n   <dbl>\n 1    91\n 2    64\n 3   243\n 4   167\n 5   123\n 6    65\n 7    71\n 8   204\n 9   110\n10   178\n# i 20 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Test whether mean is 160 or greater\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(irs, t.test(Time, mu = 160, \n                 alternative = \"greater\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  Time\nt = 1.8244, df = 29, p-value = 0.03921\nalternative hypothesis: true mean is greater than 160\n95 percent confidence interval:\n 162.8305      Inf\nsample estimates:\nmean of x \n 201.2333 \n```\n\n\n:::\n:::\n\n\n\n\nReject null; mean (for all people to complete form) greater than 160.\n\n## But, look at a graph\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(irs, aes(x = Time)) + geom_histogram(bins = 6)\n```\n\n::: {.cell-output-display}\n![](alternative_c33_files/figure-beamer/inference-3-R-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   Skewed to right.\n-   Should look at *median*, not mean.\n\n## The sign test\n\n-   But how to test whether the median is greater than 160?\n-   Idea: if the median really is 160 ($H_0$ true), the sampled values\n    from the population are equally likely to be above or below 160.\n-   If the population median is greater than 160, there will be a lot of\n    sample values greater than 160, not so many less. Idea: test\n    statistic is number of sample values greater than hypothesized\n    median.\n\n## Getting a P-value for sign test 1/3\n\n-   How to decide whether \"unusually many\" sample values are greater\n    than 160? Need a sampling distribution.\n-   If $H_0$ true, pop. median is 160, then each sample value\n    independently equally likely to be above or below 160.\n-   So number of observed values above 160 has binomial distribution\n    with $n = 30$ (number of data values) and $p = 0.5$ (160 is\n    hypothesized to be *median*).\n\n## Getting P-value for sign test 2/3\n\n-   Count values above/below 160:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nirs %>% count(Time > 160)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n  `Time > 160`     n\n  <lgl>        <int>\n1 FALSE           13\n2 TRUE            17\n```\n\n\n:::\n:::\n\n\n\n\n-   17 above, 13 below. How unusual is that? Need a *binomial table*.\n\n## Getting P-value for sign test 3/3\n\n-   R function `dbinom` gives the probability of eg. exactly 17\n    successes in a binomial with $n = 30$ and $p = 0.5$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(17, 30, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1115351\n```\n\n\n:::\n:::\n\n\n\n\n-   but we want probability of 17 *or more*, so get all of those, find\n    probability of each, and add them up:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(x=17:30) %>% \n  mutate(prob=dbinom(x, 30, 0.5)) %>% \n  summarize(total=sum(prob))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 1\n  total\n  <dbl>\n1 0.292\n```\n\n\n:::\n:::\n\n\n\n\n## ... or\n\nuse cumulative distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(17, 30, 0.5) # prob of <= 17\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8192027\n```\n\n\n:::\n:::\n\n\n\n\nand hence (note first input):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(16, 30, 0.5, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2923324\n```\n\n\n:::\n:::\n\n\n\n\nThis last is $P(X \\ge 17) = P(X > 16)$.\n\n## Using my package `smmr`\n\n-   I wrote a package `smmr` to do the sign test (and some other\n    things). Installation is non-standard:\n\n    \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"smmr\", repos = \"nxskok.r-universe.dev\")\n```\n:::\n\n\n\n    \n\n-   Then load it:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(smmr)\n```\n:::\n\n\n\n\n## `smmr` for sign test\n\n-   `smmr`'s function `sign_test` needs three inputs: a data frame, a\n    column and a null median:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsign_test(irs, Time, 160)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n   13    17 \n\n$p_values\n  alternative   p_value\n1       lower 0.8192027\n2       upper 0.2923324\n3   two-sided 0.5846647\n```\n\n\n:::\n:::\n\n\n\n\n## Comments (1/4)\n\n-   Testing whether population median *greater than* 160, so want\n    *upper-tail* P-value 0.2923. Same as before.\n-   Also get table of values above and below; this too as we got.\n\n## Comments (2/4)\n\n-   P-values are:\n\n| Test | P-value |\n|:-----|--------:|\n| $t$  |  0.0392 |\n| Sign |  0.2923 |\n\n-   These are very different: we reject a mean of 160 (in favour of the\n    mean being bigger), but clearly *fail* to reject a median of 160 in\n    favour of a bigger one.\n    \n## Comments (3/4)\n    \n-   Why is that? Obtain mean and median:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nirs %>% summarize(mean_time = mean(Time), \n                  median_time = median(Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 2\n  mean_time median_time\n      <dbl>       <dbl>\n1      201.        172.\n```\n\n\n:::\n:::\n\n\n\n\n## Comments (4/4) {.smaller}\n\n-   The mean is pulled a long way up by the right skew, and is a fair\n    bit bigger than 160.\n-   The median is quite close to 160.\n-   We ought to be trusting the sign test and not the t-test here\n    (median and not mean), and therefore there is no evidence that the\n    \"typical\" time to complete the form is longer than 160 minutes.\n-   Having said that, there are clearly some people who take a lot\n    longer than 160 minutes to complete the form, and the IRS could\n    focus on simplifying its form for these people.\n-   In this example, looking at any kind of average is not really\n    helpful; a better question might be \"do an unacceptably large\n    fraction of people take longer than (say) 300 minutes to complete\n    the form?\": that is, thinking about worst-case rather than\n    average-case.\n\n## CI for median 1/2\n\n-   The sign test does not naturally come with a confidence interval for\n    the median.\n-   So we use the \"duality\" between test and confidence interval to say:\n    the (95%) confidence interval for the median contains exactly those\n    values of the null median that would not be rejected by the\n    two-sided sign test (at $\\alpha = 0.05$).\n    \n## CI for median 2/2\n\n- Precisely: Let $C = 100(1 - \\alpha)$, so C% gives\ncorresponding CI to level-$\\alpha$ test. Then following always true.\n(Symbol $\\iff$ means \"if and only if\".)\n\n| Test decision                         |        | Confidence interval                   |\n|:--------------------------------------|:------:|:--------------------------------------|\n| Reject $H_0$ at level $\\alpha$        | $\\iff$ | $C\\%$ CI does not contain $H_0$ value |\n| Do not reject $H_0$ at level $\\alpha$ | $\\iff$ | $C\\%$ CI contains $H_0$ value         |\n\n- Idea: \n  - \"Plausible\" parameter value inside CI, not rejected;         \n  - \"Implausible\" parameter value outside CI, rejected.\n\n## The value of this\n\n-   If you have a test procedure but no corresponding CI:\n-   you make a CI by including all the parameter values that would not\n    be rejected by your test.\n-   Use:\n    -   $\\alpha = 0.01$ for a 99% CI,\n    -   $\\alpha = 0.05$ for a 95% CI,\n    -   $\\alpha = 0.10$ for a 90% CI, and so on.\n\n## For our data\n\n-   The procedure is to try some values for the null median and see\n    which ones are inside and which outside our CI.\n-   smmr has pval_sign that gets just the 2-sided P-value:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npval_sign(160, irs, Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5846647\n```\n\n\n:::\n:::\n\n\n\n\n-   Try a couple of null medians:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npval_sign(200, irs, Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3615946\n```\n\n\n:::\n\n```{.r .cell-code}\npval_sign(300, irs, Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001430906\n```\n\n\n:::\n:::\n\n\n\n\n-   So 200 inside the 95% CI and 300 outside.\n\n## Doing a whole bunch\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(d <- tibble(null_median=seq(100,300,20)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 x 1\n   null_median\n         <dbl>\n 1         100\n 2         120\n 3         140\n 4         160\n 5         180\n 6         200\n 7         220\n 8         240\n 9         260\n10         280\n11         300\n```\n\n\n:::\n:::\n\n\n\n\n## ... and then\n\n\"for each null median, run the function `pval_sign` for that null median\nand get the P-value\":\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% rowwise() %>% \n  mutate(p_value = pval_sign(null_median, irs, Time))\n```\n:::\n\n\n\n\n## Results\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 x 2\n# Rowwise: \n   null_median  p_value\n         <dbl>    <dbl>\n 1         100 0.000325\n 2         120 0.0987  \n 3         140 0.200   \n 4         160 0.585   \n 5         180 0.856   \n 6         200 0.362   \n 7         220 0.0428  \n 8         240 0.0161  \n 9         260 0.00522 \n10         280 0.00143 \n11         300 0.00143 \n```\n\n\n:::\n:::\n\n\n\n\n\n## Make it easier for ourselves\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% rowwise() %>% \n  mutate(p_value = pval_sign(null_median, irs, Time)) %>% \n  mutate(in_out = ifelse(p_value > 0.05, \"inside\", \"outside\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 x 3\n# Rowwise: \n   null_median  p_value in_out \n         <dbl>    <dbl> <chr>  \n 1         100 0.000325 outside\n 2         120 0.0987   inside \n 3         140 0.200    inside \n 4         160 0.585    inside \n 5         180 0.856    inside \n 6         200 0.362    inside \n 7         220 0.0428   outside\n 8         240 0.0161   outside\n 9         260 0.00522  outside\n10         280 0.00143  outside\n11         300 0.00143  outside\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Confidence interval for median?\n\n-   95% CI to this accuracy from 120 to 200.\n-   Can get it more accurately by looking more closely in intervals from\n    100 to 120, and from 200 to 220.\n\n## A more efficient way: bisection\n\n-   Know that top end of CI between 200 and 220:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlo <- 200 \nhi <- 220\n```\n:::\n\n\n\n\n-   Try the value halfway between: is it inside or outside?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntry <- (lo + hi) / 2\ntry\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 210\n```\n\n\n:::\n\n```{.r .cell-code}\npval_sign(try,irs,Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09873715\n```\n\n\n:::\n:::\n\n\n\n\n-   Inside, so upper end is between 210 and 220. Repeat (over):\n\n## ... bisection continued\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlo <- try\ntry <- (lo + hi) / 2\ntry\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 215\n```\n\n\n:::\n\n```{.r .cell-code}\npval_sign(try, irs, Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06142835\n```\n\n\n:::\n:::\n\n\n\n\n-   215 is inside too, so upper end between 215 and 220.\n-   Continue until have as accurate a result as you want.\n\n## Bisection automatically\n\n-   A loop, but not a `for` since we don't know how many times we're\n    going around. Keep going `while` a condition is true:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlo = 200\nhi = 220\nwhile (hi - lo > 1) { # replace 1 by desired accuracy\n  try = (hi + lo) / 2\n  ptry = pval_sign(try, irs, Time)\n  print(c(try, ptry))\n  if (ptry <= 0.05)\n    hi = try\n  else\n    lo = try\n}\n```\n:::\n\n\n\n\n## The output from this loop\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 210.00000000   0.09873715\n[1] 215.00000000   0.06142835\n[1] 217.50000000   0.04277395\n[1] 216.25000000   0.04277395\n[1] 215.62500000   0.04277395\n```\n\n\n:::\n:::\n\n\n\n\n-   215 inside, 215.625 outside. Upper end of interval to this accuracy\n    is 215.\n\n## Using smmr\n\n-   `smmr` has function `ci_median` that does this (by default 95% CI):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(irs, Time)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 119.0065 214.9955\n```\n\n\n:::\n:::\n\n\n\n\n-   Uses a more accurate bisection than we did.\n-   Or get, say, 90% CI for median:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_median(irs, Time, conf.level=0.90)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 123.0031 208.9960\n```\n\n\n:::\n:::\n\n\n\n\n-   90% CI is shorter, as it should be.\n\n## Bootstrap\n\n-   but, was the sample size (30) big enough to overcome the skewness?\n-   Bootstrap, again:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(irs$Time, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(sample = my_mean)) + \n    stat_qq() + stat_qq_line() -> g\n```\n:::\n\n\n\n\n## The sampling distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](alternative_c33_files/figure-beamer/inference-3-R-30-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   A little skewed to right, but not nearly as much as I was expecting.\n-   The $t$-test for the mean might actually be OK for these data, *if\n    the mean is what you want*.\n-   In actual data, mean and median very different; we chose to make\n    inference about the median.\n-   Thus for us it was right to use the sign test.\n\n## Two samples: Mood's median test\n\n-   If normality fails (for one or both of the groups), what do we do\n    then?\n-   Again, can compare medians: use the thought process of the sign\n    test, which does not depend on normality and is not damaged by\n    outliers.\n-   A suitable test called Mood's median test.\n-   Before we get to that, a diversion.\n\n## The chi-squared test for independence\n\nSuppose we want to know whether people are in favour of having daylight\nsavings time all year round. We ask 20 males and 20 females whether they\neach agree with having DST all year round (\"yes\") or not (\"no\"). \n\n## Some of the data:\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/dst.txt\"\ndst <- read_delim(my_url,\" \")\ndst %>% slice_sample(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 x 2\n   gender agree\n   <chr>  <chr>\n 1 male   yes  \n 2 male   yes  \n 3 female yes  \n 4 male   yes  \n 5 male   yes  \n 6 male   yes  \n 7 male   yes  \n 8 male   no   \n 9 male   yes  \n10 male   yes  \n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## ... continued\n\nCount up individuals in each category combination, and arrange in\ncontingency table:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab <- with(dst, table(gender, agree))\ntab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        agree\ngender   no yes\n  female 11   9\n  male    3  17\n```\n\n\n:::\n:::\n\n\n\n\n-   Most of the males say \"yes\", but the females are about evenly split.\n-   Looks like males more likely to say \"yes\", ie. an association\n    between gender and agreement.\n  \n## ... continued\n    \n-   Test an $H_0$ of \"no association\" (\"independence\") vs. alternative\n    that there is really some association.\n-   Done with `chisq.test`.\n\n## ...And finally\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(tab, correct=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab\nX-squared = 7.033, df = 1, p-value = 0.008002\n```\n\n\n:::\n:::\n\n\n\n\n-   Reject null hypothesis of no association (P-value 0.008)\n-   therefore there is a difference in rates of agreement between (all)\n    males and females (or that gender and agreement are associated).\n-   Same answers as by hand.\n    (Omitting `correct = FALSE` uses \"Yates correction\".\n\n## Mood's median test\n\n-   Earlier: compare medians of two groups.\n-   Sign test: count number of values above and below something (there,\n    hypothesized median).\n-   Mood's median test:\n    -   Find \"grand median\" of all the data, regardless of group\n    -   Count data values in each group above/below grand median.\n    -   Make contingency table of group vs. above/below.\n    -   Test for association.\n    \n## Why it works    \n    \n-   If group medians equal, each group should have about half its\n    observations above/below grand median. If not, one group will be\n    mostly above grand median and other below.\n\n## Mood's median test for reading data\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n-   Find overall median score:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkids %>% summarize(med=median(score)) %>% pull(med) -> m\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 47\n```\n\n\n:::\n:::\n\n\n\n\n-   Make table of above/below vs. group:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab <- with(kids, table(group, score > m))\ntab\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     \ngroup FALSE TRUE\n    c    15    8\n    t     7   14\n```\n\n\n:::\n:::\n\n\n\n\n-   Treatment group scores mostly above median, control group scores\n    mostly below, as expected.\n\n## The test\n\n-   Do chi-squared test:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(tab, correct=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab\nX-squared = 4.4638, df = 1, p-value = 0.03462\n```\n\n\n:::\n:::\n\n\n\n\n-   Two-sided (tests for any association).\n-   Here, is reading method *better*? (one-sided).\n-   Most of treatment children above overall median, so halve P-value to get 0.017.\n-   Again, children learn to read better using new\n    method.\n\n## Or by smmr\n\n-   `median_test` does the whole thing:\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_test(kids,score,group)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$grand_median\n[1] 47\n\n$table\n     above\ngroup above below\n    c     8    15\n    t    14     7\n\n$test\n       what      value\n1 statistic 4.46376812\n2        df 1.00000000\n3   P-value 0.03462105\n```\n\n\n:::\n:::\n\n\n\n\n-   P-value again two-sided.\n\n\\normalsize\n\n## Comments 1/2\n\n-   P-value 0.013 for (1-sided) t-test, 0.017 for (1-sided) Mood median\n    test.\n-   Like the sign test, Mood's median test doesn't use the data very\n    efficiently (only, is each value above or below grand median).\n-   Thus, if we can justify doing *t*-test, we should do it. This is the\n    case here.\n\n## Comments 2/2\n\n-   The *t*-test will usually give smaller P-value because it uses the\n    data more efficiently.\n-   The time to use Mood's median test is if we are definitely unhappy\n    with the normality assumption (and thus the t-test P-value is not to\n    be trusted).\n-   There is no obvious way to get a confidence interval for the difference between the two medians.\n\n\n## Matched pairs: the pain relief data\n\nValues aligned in columns:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \n  \"http://ritsokiguess.site/datafiles/analgesic.txt\"\npain <- read_table(my_url)\npain %>% mutate(diff = druga - drugb) -> pain\nglimpse(pain)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 12\nColumns: 4\n$ subject <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ druga   <dbl> 2.0, 3.6, 2.6, 2.6, 7.3, 3.4, 14.9, 6.6, 2.3, 2.0, 6.8, 8.5\n$ drugb   <dbl> 3.5, 5.7, 2.9, 2.4, 9.9, 3.3, 16.7, 6.0, 3.8, 4.0, 9.1, 20.9\n$ diff    <dbl> -1.5, -2.1, -0.3, 0.2, -2.6, 0.1, -1.8, 0.6, -1.5, -2.0, -2.3,~\n```\n\n\n:::\n:::\n\n\n\n\n\n## Assessing normality\n\n-   Matched pairs analyses assume (theoretically) that differences\n    normally distributed.\n-   How to assess normality? A normal quantile plot.\n\n## The normal quantile plot (of differences)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pain, aes(sample = diff)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](alternative_c33_files/figure-beamer/inference-4b-R-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n-   Points should follow the straight line. Bottom left one way off, so\n    normality questionable here: outlier.\n\n## What to do instead?\n\n-   Matched pairs $t$-test based on one sample of differences\n-   the differences not normal (enough)\n-   so do *sign test* on differences, null median 0:\n\n## ... continued\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsign_test(pain, diff, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$above_below\nbelow above \n    9     3 \n\n$p_values\n  alternative    p_value\n1       lower 0.07299805\n2       upper 0.98071289\n3   two-sided 0.14599609\n```\n\n\n:::\n:::\n\n\n\n\n- No evidence of any difference between the drugs (that the median difference is not zero).\n\n## Did we really need to worry about that outlier?\n\nBootstrap sampling distribution of sample mean differences:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(pain$diff, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) %>% \n  ggplot(aes(sample = my_mean)) + stat_qq() + stat_qq_line() -> g\n```\n:::\n\n\n\n\n## The normal quantile plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](alternative_c33_files/figure-beamer/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\nYes we did need to worry; this is clearly skewed left and not normal.\n\n",
    "supporting": [
      "alternative_c33_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}