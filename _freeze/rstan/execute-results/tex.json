{
  "hash": "fc3e47fca30f8a0b4318463295e5d474",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Statistics with Stan\"\nexecute:\n  echo: true\n---\n\n\n\n\n## Packages for this section\n\nInstallation instructions for the last three of these are below.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n## Installation 1/2\n\n-   `cmdstanr`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"cmdstanr\", \n                 repos = c(\"https://stan-dev.r-universe.dev\",\n                           \"https://cloud.r-project.org\"))\n```\n:::\n\n\n\n\n-   `posterior` and `bayesplot`, from the same place:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"posterior\", \n                 repos = c(\"https://stan-dev.r-universe.dev\",\n                           \"https://cloud.r-project.org\"))\ninstall.packages(\"bayesplot\", \n                 repos = c(\"https://stan-dev.r-universe.dev\",\n                           \"https://cloud.r-project.org\"))\n```\n:::\n\n\n\n\n## Installation 2/2\n\nThen, to check that you have the C++ stuff needed to compile Stan code:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_cmdstan_toolchain()\n```\n:::\n\n\n\n\nwhich should produce output like `The C++ toolchain required for CmdStan is setup properly!`, and then:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall_cmdstan(cores = 6)\n```\n:::\n\n\n\n\nIf you happen to know how many cores (processors) your computer has,\ninsert the appropriate number. (My new laptop has 8 and my desktop 6.)\n\nAll of this is done once. If you have problems, go [here\n(link)](https://mc-stan.org/cmdstanr/articles/cmdstanr.html).\n\n## Bayesian and frequentist inference 1/2\n\n-   The inference philosophy that we have learned so far says that:\n    -   parameters to be estimated are *fixed* but *unknown*\n    -   Data random; if we took another sample we'd get different data.\n-   This is called \"frequentist\" or \"repeated-sampling\" inference.\n\n## Bayesian and frequentist inference 2/2\n\n-   Bayesian inference says:\n    -   *parameters* are random, *data* is *given*\n-   Ingredients:\n    -   **prior distribution**: distribution of parameters before seeing\n        data.\n    -   **likelihood**: model for data if the parameters are known\n    -   **posterior distribution**: distribution of parameters *after*\n        seeing data.\n\n## Distribution of parameters\n\n-   Instead of having a point or interval estimate of a parameter, we\n    have an entire distribution\n-   so in Bayesian statistics we can talk about eg.\n    -   probability that a parameter is bigger than some value\n    -   probability that a parameter is close to some value\n    -   probability that one parameter is bigger than another\n-   Name comes from Bayes' Theorem, which here says\n\n> posterior is proportional to likelihood times prior\n\n-   more discussion about this is in [**a blog\n    post**](http://ritsokiguess.site/docs/2018/02/28/working-my-way-back-to-you-a-re-investigation-of-rstan/).\n\n## An example\n\n-   Suppose we have these (integer) observations:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(x <- c(0, 4, 3, 6, 3, 3, 2, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 4 3 6 3 3 2 4\n```\n\n\n:::\n:::\n\n\n\n\n-   Suppose we believe that these come from a Poisson distribution with\n    a mean $\\lambda$ that we want to estimate.\n-   We need a prior distribution for $\\lambda$. I will (for some reason)\n    take a $Weibull$ distribution with parameters 1.1 and 6, that has\n    quartiles 2 and 6. Normally this would come from your knowledge of\n    the data-generating *process*.\n-   The Poisson likelihood can be written down (see over).\n\n## Some algebra\n\n-   We have $n=8$ observations $x_i$, so the Poisson likelihood is\n    proportional to\n\n$$ \\prod_{i=1}^n e^{-\\lambda} \\lambda^{x_i} = e^{-n\\lambda} \\lambda^S, $$\nwhere $S=\\sum_{i=1}^n x_i$.\n\n-   then you write the Weibull prior density (as a function of\n    $\\lambda$):\n\n$$ C (\\lambda/6)^{0.1} e^{-(\\lambda/6)^{1.1}}  $$ where $C$ is a\nconstant.\n\n-   and then you multiply these together and try to recognize the\n    distributional form. Only, here you can't. The powers 0.1 and 1.1\n    get in the way.\n\n## Sampling from the posterior distribution\n\n-   Wouldn't it be nice if we could just *sample* from the posterior\n    distribution? Then we would be able to compute it as accurately as\n    we want.\n\n-   Metropolis and Hastings: devise a Markov chain (C62) whose limiting\n    distribution is the posterior you want, and then sample from that\n    Markov chain (easy), allowing enough time to get close enough to the\n    limiting distribution.\n\n-   Stan: uses a modern variant that is more efficient (called\n    Hamiltonian Monte Carlo), implemented in R packages `cmdstanr`.\n\n-   Write Stan code in a file, compile it and sample from it.\n\n## Components of Stan code: the model\n\n```         \nmodel {\n  // likelihood\n  x ~ poisson(lambda);\n}\n```\n\nThis is how you say \"$X$ has a Poisson distribution with mean\n$\\lambda$\". **Note that lines of Stan code have semicolons on the end.**\n\n## Components of Stan code: the prior distribution\n\n```         \nmodel {\n  // prior\n  lambda ~ weibull(1.1, 6);\n  // likelihood\n  x ~ poisson(lambda);\n}\n```\n\n## Components of Stan code: data and parameters\n\n-   first in the Stan code:\n\n```         \ndata {\n  array[8] int x;\n}\n\nparameters {\n  real<lower=0> lambda;\n}\n```\n\n## Compile and sample from the model 1/2\n\n-   compile \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1 <- cmdstan_model(\"poisson1.stan\")\n```\n:::\n\n\n\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n// Estimating Poisson mean\n\ndata {\n  array[8] int x;\n}\n\nparameters {\n  real<lower=0> lambda;\n}\n\nmodel {\n  // prior\n  lambda ~ weibull(1.1, 6);\n  // likelihood\n  x ~ poisson(lambda);\n}\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Compile and sample from the model 2/2\n\n-   set up data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1_data <- list(x = x)\npoisson1_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$x\n[1] 0 4 3 6 3 3 2 4\n```\n\n\n:::\n:::\n\n\n\n\n-   sample (output is (very) long)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1_fit <- poisson1$sample(data = poisson1_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.7 seconds.\n```\n\n\n:::\n:::\n\n\n\n\n## The output\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n variable mean median   sd  mad   q5  q95 rhat ess_bulk ess_tail\n   lp__   3.77   4.04 0.70 0.30 2.35 4.26 1.00     1999     2396\n   lambda 3.17   3.14 0.61 0.60 2.23 4.22 1.00     1533     2065\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Comments\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n-   This summarizes the posterior distribution of $\\lambda$\n-   the posterior mean is 3.17\n-   with a 90% posterior interval of 2.23 to 4.22.\n-   The probability that $\\lambda$ is between these two values really is\n    90%.\n\n## Making the code more general\n\n-   The coder in you is probably offended by hard-coding the sample size\n    and the parameters of the prior distribution. More generally:\n\n```         \ndata {\n  int<lower=1> n;\n  real<lower=0> a;\n  real<lower=0> b;\n  array[n] int x;\n}\n...\nmodel {\n// prior\nlambda ~ weibull(a, b);\n// likelihood\nx ~ poisson(lambda);\n}\n```\n\n## Set up again and sample:\n\n-   Compile again:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2 <- cmdstan_model(\"poisson2.stan\")\n```\n:::\n\n\n\n\n-   set up the data again including the new things we need:\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_data <- list(x = x, n = length(x), a = 1.1, b = 6)\npoisson2_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$x\n[1] 0 4 3 6 3 3 2 4\n\n$n\n[1] 8\n\n$a\n[1] 1.1\n\n$b\n[1] 6\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Sample again\n\nOutput should be the same (to within randomness):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_fit <- poisson2$sample(data = poisson2_data)\n```\n:::\n\n\n\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n variable mean median   sd  mad   q5  q95 rhat ess_bulk ess_tail\n   lp__   3.75   4.02 0.70 0.33 2.35 4.26 1.00     1784     2178\n   lambda 3.21   3.17 0.63 0.63 2.25 4.33 1.00     1363     1768\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Picture of posterior\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_hist(poisson2_fit$draws(\"lambda\"), binwidth = 0.25)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Extracting actual sampled values\n\nA little awkward at first:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(poisson2_fit$draws())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n 'draws_array' num [1:1000, 1:4, 1:2] 4.19 3.94 4.09 4.2 3.93 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ iteration: chr [1:1000] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ chain    : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ variable : chr [1:2] \"lp__\" \"lambda\"\n```\n\n\n:::\n:::\n\n\n\n\nA 3-dimensional array. A dataframe would be much better.\n\n## Sampled values as dataframe\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(poisson2_fit$draws()) %>% \n  as_tibble() -> poisson2_draws\npoisson2_draws\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 x 5\n    lp__ lambda .chain .iteration .draw\n   <dbl>  <dbl>  <int>      <int> <int>\n 1  4.19   3.43      1          1     1\n 2  3.94   2.72      1          2     2\n 3  4.09   2.84      1          3     3\n 4  4.20   3.41      1          4     4\n 5  3.93   3.72      1          5     5\n 6  4.17   2.93      1          6     6\n 7  4.05   2.81      1          7     7\n 8  4.13   3.53      1          8     8\n 9  2.46   2.15      1          9     9\n10  3.08   2.33      1         10    10\n# i 3,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Posterior predictive distribution\n\n-   Another use for the actual sampled values is to see what kind of\n    *response* values we might get in the future. This should look\n    something like our data. For a Poisson distribution, the response\n    values are integers:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_draws %>% \n  rowwise() %>% \n  mutate(xsim = rpois(1, lambda)) -> d\n```\n:::\n\n\n\n\n## The simulated posterior distribution (in `xsim`)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% select(lambda, xsim)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 x 2\n# Rowwise: \n   lambda  xsim\n    <dbl> <int>\n 1   3.43     4\n 2   2.72     4\n 3   2.84     4\n 4   3.41     3\n 5   3.72     2\n 6   2.93     2\n 7   2.81     4\n 8   3.53     4\n 9   2.15     2\n10   2.33     2\n# i 3,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Comparison\n\nOur actual data values were these:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0 4 3 6 3 3 2 4\n```\n\n\n:::\n:::\n\n\n\n\n-   None of these are very unlikely according to our posterior\n    predictive distribution, so our model is believable.\n-   Or make a plot: a bar chart with the data on it as well (over):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = xsim)) + geom_bar() +\n  geom_dotplot(data = tibble(x), aes(x = x), binwidth = 1) +\n  scale_y_continuous(NULL, breaks = NULL) -> g\n```\n:::\n\n\n\n\n-   This also shows that the distribution of the data conforms well\n    enough to the posterior predictive distribution (over).\n\n## The plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Do they have the same distribution?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqplot(d$xsim, x, plot.it = FALSE) %>% as_tibble() -> dd\ndd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 2\n      x     y\n  <dbl> <dbl>\n1     0     0\n2     1     2\n3     2     3\n4     3     3\n5     3     3\n6     4     4\n7     5     4\n8    13     6\n```\n\n\n:::\n:::\n\n\n\n\n## The plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dd, aes(x=x, y=y)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nthe observed zero is a bit too small compared to expected (from the\nposterior), but the other points seem pretty well on a line.\n\n## Analysis of variance, the Bayesian way\n\nRecall the jumping rats data:\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \n  \"http://ritsokiguess.site/datafiles/jumping.txt\"\nrats0 <- read_delim(my_url, \" \")\nrats0\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 x 2\n   group   density\n   <chr>     <dbl>\n 1 Control     611\n 2 Control     621\n 3 Control     614\n 4 Control     593\n 5 Control     593\n 6 Control     653\n 7 Control     600\n 8 Control     554\n 9 Control     603\n10 Control     569\n# i 20 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Our aims here\n\n-   Estimate the mean bone density of all rats under each of the\n    experimental conditions\n-   Model: given the group means, each observation normally distributed\n    with common variance $\\sigma^2$\n-   Three parameters to estimate, plus the common variance.\n-   Obtain posterior distributions for the group means.\n-   Ask whether the posterior distributions of these means are\n    sufficiently different.\n\n## Numbering the groups 1/2\n\n-   Stan doesn't handle categorical variables (everything is `real` or\n    `int`).\n-   Turn the groups into group *numbers* first.\n-   Take opportunity to put groups in logical order:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrats0 %>% mutate(\n  group_fct = fct_inorder(group),\n  group_no = as.integer(group_fct)\n) -> rats\n```\n:::\n\n\n\n\n## Numbering the groups 2/2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 x 4\n   group   density group_fct group_no\n   <chr>     <dbl> <fct>        <int>\n 1 Control     611 Control          1\n 2 Control     621 Control          1\n 3 Control     614 Control          1\n 4 Control     593 Control          1\n 5 Control     593 Control          1\n 6 Control     653 Control          1\n 7 Control     600 Control          1\n 8 Control     554 Control          1\n 9 Control     603 Control          1\n10 Control     569 Control          1\n# i 20 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Plotting the data 1/2\n\nMost obviously, boxplots:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rats, aes(x = group_fct, y = density)) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-26-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Plotting the data 2/2\n\nAnother way: density plot (smoothed out histogram); can distinguish\ngroups by colours:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rats, aes(x = density, fill = group_fct)) +\n  geom_density(alpha = 0.6)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/density_plot-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## The procedure\n\n-   For each observation, find out which (numeric) group it belongs to,\n-   then model it as having a normal distribution with that group's mean\n    and the common variance.\n-   Stan does `for` loops.\n\n## The model part\n\nSuppose we have `n_obs` observations:\n\n```         \nmodel {\n  // likelihood\n  for (i in 1:n_obs) {\n    g = group_no[i];\n    density[i] ~ normal(mu[g], sigma);\n  }\n}\n```\n\n## The variables here {.scrollable}\n\n-   `n_obs` is data.\n-   `g` is a temporary integer variable only used here\n-   `i` is only used in the loop (integer) and does not need to be\n    declared\n-   `density` is data, a real vector of length `n_obs`\n-   `mu` is a parameter, a real vector of length 3 (3 groups)\n-   `sigma` is a real parameter\n\n`mu` and `sigma` need prior distributions:\n\n-   for `mu`, each component independently normal with mean 600 and SD\n    50 (my guess at how big and variable they will be)\n-   for `sigma`, chi-squared with 50 df (my guess at typical amount of\n    variability from obs to obs)\n\n## Complete the `model` section:\n\n```         \nmodel {\n  int g;\n  // priors\n  mu ~ normal(600, 50);\n  sigma ~ chi_square(50);\n  // likelihood\n  for (i in 1:n_obs) {\n    g = group_no[i];\n    density[i] ~ normal(mu[g], sigma);\n  }\n}\n```\n\n## Parameters\n\nThe elements of `mu`, one per group, and also `sigma`, scalar, lower\nlimit zero:\n\n```         \nparameters {\n  array[n_group] real mu;\n  real<lower=0> sigma;\n}\n```\n\n-   Declare `sigma` to have lower limit zero here, so that the sampling\n    runs smoothly.\n-   declare `n_group` in data section\n\n## Data\n\nEverything else:\n\n```         \ndata {\n  int n_obs;\n  int n_group;\n  array[n_obs] real density;\n  array[n_obs] int<lower=1, upper=n_group> group_no;\n}\n```\n\n## Compile\n\nArrange these in order data, parameters, model in file `anova.stan`,\nthen:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova <- cmdstan_model(\"anova.stan\")\n```\n:::\n\n\n\n\n## Set up data and sample\n\nSupply values for *everything* declared in `data`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_data <- list(\n  n_obs = 30,\n  n_group = 3,\n  density = rats$density,\n  group_no = rats$group_no\n)\nanova_fit <- anova$sample(data = anova_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n```\n\n\n:::\n:::\n\n\n\n\n## Check that the sampling worked properly\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_fit$cmdstan_diagnose()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nRank-normalized split effective sample size satisfactory for all parameters.\n\nRank-normalized split R-hat values satisfactory for all parameters.\n\nProcessing complete, no problems detected.\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Look at the results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n    lp__  -41.02 -40.64 1.48 1.26 -43.96 -39.32 1.00     1827     2574\n    mu[1] 601.00 601.26 8.85 8.56 586.09 615.69 1.00     4682     3241\n    mu[2] 611.95 611.95 9.05 8.72 597.20 626.65 1.00     3983     2569\n    mu[3] 637.54 637.61 8.79 8.63 623.11 652.17 1.00     4368     2760\n    sigma  28.48  28.12 4.23 4.08  22.21  36.20 1.00     3623     2632\n```\n\n\n:::\n:::\n\n\n\n\n## Comments\n\n-   The posterior 90% intervals for control (group 1) and highjump\n    (group 3) do not quite overlap, suggesting that these exercise\n    groups really are different.\n-   Bayesian approach does not normally do tests: look at posterior\n    distributions and decide whether they are different enough to be\n    worth treating as different.\n\n## Plotting the posterior distributions for the `mu`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_hist(anova_fit$draws(\"mu\"), binwidth = 5)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-29-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Extract the sampled values\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(anova_fit$draws()) %>% as_tibble() -> anova_draws\nanova_draws\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4,000 x 8\n    lp__ `mu[1]` `mu[2]` `mu[3]` sigma .chain .iteration .draw\n   <dbl>   <dbl>   <dbl>   <dbl> <dbl>  <int>      <int> <int>\n 1 -42.8    588.    622.    619.  31.6      1          1     1\n 2 -41.5    603.    595.    630.  32.3      1          2     2\n 3 -42.3    603.    631.    635.  24.1      1          3     3\n 4 -41.4    606.    614.    643.  36.3      1          4     4\n 5 -40.5    604.    616.    630.  22.2      1          5     5\n 6 -40.0    599.    613.    645.  31.4      1          6     6\n 7 -40.8    586.    606.    634.  27.0      1          7     7\n 8 -42.8    616.    602.    637.  37.3      1          8     8\n 9 -40.3    590.    606.    632.  28.9      1          9     9\n10 -40.5    616.    609.    638.  27.0      1         10    10\n# i 3,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Estimated probability that $\\mu_3 > \\mu_1$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_draws %>% \n  count(`mu[3]`>`mu[1]`) %>% \n  mutate(prob = n/sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  `\\`mu[3]\\` > \\`mu[1]\\``     n  prob\n  <lgl>                   <int> <dbl>\n1 FALSE                       8 0.002\n2 TRUE                     3992 0.998\n```\n\n\n:::\n:::\n\n\n\n\nHigh jumping group almost certainly has larger mean than control group.\n\n## Compare lowjump and control the same way\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_draws %>% \n  count(`mu[2]`>`mu[1]`) %>% \n  mutate(prob = n/sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  `\\`mu[2]\\` > \\`mu[1]\\``     n  prob\n  <lgl>                   <int> <dbl>\n1 FALSE                     746 0.186\n2 TRUE                     3254 0.814\n```\n\n\n:::\n:::\n\n\n\n\nLikely that lowjump mean higher than control mean, but not a certainty.\n\n## More organizing\n\n-   for another plot\n    -   make longer\n    -   give `group` values their proper names back\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_draws %>% \n  pivot_longer(starts_with(\"mu\"), \n               names_to = \"group\", \n               values_to = \"bone_density\") %>% \n  mutate(group = fct_recode(group,\n    Control = \"mu[1]\",\n    Lowjump = \"mu[2]\",\n    Highjump = \"mu[3]\"\n  )) -> sims\n```\n:::\n\n\n\n\n## What we have now:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsims \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12,000 x 7\n    lp__ sigma .chain .iteration .draw group    bone_density\n   <dbl> <dbl>  <int>      <int> <int> <fct>           <dbl>\n 1 -42.8  31.6      1          1     1 Control          588.\n 2 -42.8  31.6      1          1     1 Lowjump          622.\n 3 -42.8  31.6      1          1     1 Highjump         619.\n 4 -41.5  32.3      1          2     2 Control          603.\n 5 -41.5  32.3      1          2     2 Lowjump          595.\n 6 -41.5  32.3      1          2     2 Highjump         630.\n 7 -42.3  24.1      1          3     3 Control          603.\n 8 -42.3  24.1      1          3     3 Lowjump          631.\n 9 -42.3  24.1      1          3     3 Highjump         635.\n10 -41.4  36.3      1          4     4 Control          606.\n# i 11,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Density plots of posterior mean distributions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sims, aes(x = bone_density, fill = group)) + \n  geom_density(alpha = 0.6)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Posterior predictive distributions\n\nRandomly sample from posterior means and SDs in `sims`. There are 12000\nrows in `sims`:\n\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsims %>% mutate(sim_data = rnorm(12000, bone_density,\n                                 sigma)) -> ppd\nppd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12,000 x 8\n    lp__ sigma .chain .iteration .draw group    bone_density sim_data\n   <dbl> <dbl>  <int>      <int> <int> <fct>           <dbl>    <dbl>\n 1 -42.8  31.6      1          1     1 Control          588.     567.\n 2 -42.8  31.6      1          1     1 Lowjump          622.     608.\n 3 -42.8  31.6      1          1     1 Highjump         619.     616.\n 4 -41.5  32.3      1          2     2 Control          603.     579.\n 5 -41.5  32.3      1          2     2 Lowjump          595.     572.\n 6 -41.5  32.3      1          2     2 Highjump         630.     640.\n 7 -42.3  24.1      1          3     3 Control          603.     611.\n 8 -42.3  24.1      1          3     3 Lowjump          631.     650.\n 9 -42.3  24.1      1          3     3 Highjump         635.     650.\n10 -41.4  36.3      1          4     4 Control          606.     582.\n# i 11,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Compare posterior predictive distribution with actual data\n\n-   Check that the model works: distributions of data similar to what\n    we'd predict\n-   Idea: make plots of posterior predictive distribution, and plot\n    actual data as points on them\n-   Use facets, one for each treatment group:\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_binwidth <- 15\nggplot(ppd, aes(x = sim_data)) +\n  geom_histogram(binwidth = my_binwidth) +\n  geom_dotplot(\n    data = rats, aes(x = density),\n    binwidth = my_binwidth\n  ) +\n  facet_wrap(~group) +\n  scale_y_continuous(NULL, breaks = NULL) -> g\n```\n:::\n\n\n\n\n\\normalsize\n\n-   See (over) that the data values are mainly in the middle of the\n    predictive distributions.\n-   Even for the control group that had outliers.\n\n## The plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-35-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Extensions\n\n-   if you want a different model other than normal, change distribution\n    in `model` section\n-   if you want to allow unequal spreads, create `sigma[n_group]` and in\n    model `density[i] ~ normal(mu[g], sigma[g]);`\n-   Stan will work just fine after you recompile\n-   very flexible.\n-   Typical modelling strategy: start simple, add complexity as\n    warranted by data.\n",
    "supporting": [
      "rstan_files/figure-beamer"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}