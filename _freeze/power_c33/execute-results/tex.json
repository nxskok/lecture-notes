{
  "hash": "d26669e13aec87aa07ea662ef7b18027",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Power of hypothesis tests\"\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n## Packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n## Errors in testing\n\nWhat can happen:\n\n|                | Decision          |                 |\n|:---------------|:------------------|:----------------|\n| **Truth**      | **Do not reject** | **Reject null** |\n| **Null true**  | Correct           | Type I error    |\n| **Null false** | Type II error     | Correct         |\n\nTension between truth and decision about truth (imperfect).\n\n## ... continued\n\n-   Prob. of type I error denoted $\\alpha$. Usually fix $\\alpha$, eg.\n    $\\alpha = 0.05$.\n-   Prob. of type II error denoted $\\beta$. Determined by the planned\n    experiment. Low $\\beta$ good.\n-   Prob. of not making type II error called **power** (= $1 - \\beta$).\n    *High* power good.\n\n## Power 1/2\n\n-   Suppose $H_0 : \\theta = 10$, $H_a : \\theta \\ne 10$ for some\n    parameter $\\theta$.\n-   Suppose $H_0$ wrong. What does that say about $\\theta$?\n-   Not much. Could have $\\theta = 11$ or $\\theta = 8$ or\n    $\\theta = 496$. In each case, $H_0$ wrong.\n    \n## Power 2/2\n    \n-   How likely a type II error is depends on what $\\theta$ is:\n    -   If $\\theta = 496$, should reject $H_0 : \\theta = 10$\n        even for small sample, so $\\beta$ small (power large).\n    -   If $\\theta = 11$, hard to reject $H_0$ even with\n        large sample, so $\\beta$ would be larger (power smaller).\n-   Power depends on true parameter value, and on sample size.\n-   So we play \"what if\": \"if $\\theta$ were 11 (or 8 or 496), what would\n    power be?\".\n\n## Figuring out power 1/2\n\n-   Time to figure out power is before you collect any data, as part of\n    planning process.\n-   Need to have idea of what kind of departure from null hypothesis of\n    interest to you, eg. average improvement of 5 points on reading test\n    scores. (Subject-matter decision, not statistical one.)\n\n## Figuring out power 2/2\n\n-   Then, either:\n    -   \"I have this big a sample and this big a departure I want to\n        detect. What is my power for detecting it?\"\n    -   \"I want to detect this big a departure with this much power. How\n        big a sample size do I need?\"\n\n## How to understand/estimate power?\n\n-   Suppose we test $H_0 : \\mu = 10$ against $H_a : \\mu \\ne 10$, where\n    $\\mu$ is population mean.\n-   Suppose in actual fact, $\\mu = 8$, so $H_0$ is wrong. We want to\n    reject it. How likely is that to happen?\n-   Need population SD (take $\\sigma = 4$) and sample size (take\n    $n = 15$). In practice, get $\\sigma$ from pilot/previous study, and\n    take the $n$ we plan to use.\n-   Idea: draw a random sample from the true distribution, test whether\n    its mean is 10 or not.\n-   Repeat previous step \"many\" times: simulation.\n\n## Making it go\n\n-   Random sample of 15 normal observations with mean 8 and SD 4:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(15, 8, 4)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 14.487469  5.014611  6.924277  5.201860  8.852952 10.835874  3.686684\n [8] 11.165242  8.016188 12.383518  1.378099  3.172503 13.074996 11.353573\n[15]  5.015575\n```\n\n\n:::\n:::\n\n\n\n\n-   Test whether `x` from population with mean 10 or not (over):\n\n## ...continued\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(x, mu = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  x\nt = -1.8767, df = 14, p-value = 0.08157\nalternative hypothesis: true mean is not equal to 10\n95 percent confidence interval:\n  5.794735 10.280387\nsample estimates:\nmean of x \n 8.037561 \n```\n\n\n:::\n:::\n\n\n\n\n-   P-value 0.081, so fail to reject the mean being 10 (a Type II error).\n\n## or get just P-value\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nans <- t.test(x, mu = 10)\nans$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0815652\n```\n\n\n:::\n:::\n\n\n\n\n## Run this lots of times via simulation\n\n-   draw random samples from the truth\n-   test that $\\mu = 10$\n-   get P-value\n-   Count up how many of the P-values are 0.05 or less.\n\n## In code\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(15, 8, 4))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 10))) %>% \n  mutate(p_val = t_test$p.value) %>% \n  count(p_val <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_val <= 0.05`     n\n  <lgl>           <int>\n1 FALSE             578\n2 TRUE              422\n```\n\n\n:::\n:::\n\n\n\n\nWe correctly rejected 422 times out of 1000, so the estimated power is\n0.422.\n\n## Try again with bigger sample\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:1000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(40, 8, 4))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 10))) %>% \n  mutate(p_val = t_test$p.value) %>% \n  count(p_val <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_val <= 0.05`     n\n  <lgl>           <int>\n1 FALSE             119\n2 TRUE              881\n```\n\n\n:::\n:::\n\n\n\n \nPower is (much) larger with a bigger sample. \n \n## How accurate is my simulation?\n\n- At our chosen $\\alpha$, each simulated test independently either rejects or not with some probability $p$ that I am trying to estimate (the power)\n- Estimating a population probability using the sample proportion (the number of simulated rejections out of the number of simulated tests)\n- hence, `prop.test`.\n- inputs: number of rejections, number of simulations.\n\n## Sample size 15, rejected 422 times\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(422, 1000)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  422 out of 1000, null probability 0.5\nX-squared = 24.025, df = 1, p-value = 9.509e-07\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.3912521 0.4533546\nsample estimates:\n    p \n0.422 \n```\n\n\n:::\n:::\n\n\n\n\n95% CI for power: 0.391 to 0.453\n\n## To estimate power more accurately\n\n- Run more *simulations*:\n\nChange 1000 to eg 10,000:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(rnorm(15, 8, 4))) %>% \n  mutate(t_test = list(t.test(my_sample, mu = 10))) %>% \n  mutate(p_val = t_test$p.value) %>% \n  count(p_val <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `p_val <= 0.05`     n\n  <lgl>           <int>\n1 FALSE            5647\n2 TRUE             4353\n```\n\n\n:::\n:::\n\n\n\n\n## Accuracy of power now\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop.test(4353, 10000)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t1-sample proportions test with continuity correction\n\ndata:  4353 out of 10000, null probability 0.5\nX-squared = 167.18, df = 1, p-value < 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4255594 0.4450905\nsample estimates:\n     p \n0.4353 \n```\n\n\n:::\n:::\n\n\n\n\n0.426 to 0.445, about factor $\\sqrt{10}$ shorter because number of simulations 10 times bigger.\n\n\n## Calculating power 1/2\n\n-   Simulation approach very flexible: will work for any test. But\n    answer different each time because of randomness.\n-   In some cases, for example 1-sample and 2-sample t-tests, power can\n    be calculated.\n-   `power.t.test`. \n\n\n## Calculating power 2/2\n\n- Input `delta` is difference between null and true\n    mean:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n = 15, delta = 10-8, sd = 4, type = \"one.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 15\n          delta = 2\n             sd = 4\n      sig.level = 0.05\n          power = 0.4378466\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n\n\n## Comparison of results\n\n| Method             | Power  |\n|:-------------------|:-------|\n| Simulation (10000) | 0.4353 |\n| **`power.t.test`** | 0.4378 |\n\n-   Simulation power is similar to calculated power; to get more\n    accurate value, repeat more times (eg. 100,000 instead of 10,000),\n    which takes longer.\n-   With this small a sample size, the power is not great. With a bigger\n    sample, the sample mean should be closer to 8 most of the time, so\n    would reject $H_0 : \\mu = 10$ more often.\n\n## Calculating required sample size\n\n-   Often, when planning a study, we do not have a particular sample\n    size in mind. Rather, we want to know how big a sample to take. This\n    can be done by asking how big a sample is needed to achieve a\n    certain power.\n-   The simulation approach does not work naturally with this, since you\n    have to supply a sample size.\n    -   For that, you try different sample sizes until you get power\n        close to what you want.\n-   For the power-calculation method, you supply a value for the power,\n    but leave the sample size missing.\n\n## Using power.t.test\n\n-   Re-use the same problem: $H_0 : \\mu = 10$ against 2-sided\n    alternative, true $\\mu = 8$, $\\sigma = 4$, but now aim for power\n    0.80.\n-   No `n=`, replaced by a `power=`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(power=0.80, delta=10-8, sd=4, type=\"one.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 33.3672\n          delta = 2\n             sd = 4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n\n\n\n\n-   Sample size must be a whole number, so round up to 34 (to get at\n    least as much power as you want).\n\n## Power curves\n\n-   Rather than calculating power for one sample size, or sample size\n    for one power, might want a picture of relationship between sample\n    size and power.\n-   Or, likewise, picture of relationship between difference between\n    true and null-hypothesis means and power.\n-   Called power curve.\n-   Build and plot it yourself.\n\n\n## Building it:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(n=seq(10, 100, 10)) %>% rowwise() %>%\n  mutate(power_output = \n           list(power.t.test(n = n, delta = 10-8, sd = 4, \n                             type = \"one.sample\"))) %>% \n  mutate(power = power_output$power) %>% \n  ggplot(aes(x=n, y=power)) + geom_point() + geom_line() +\n    geom_hline(yintercept=1, linetype=\"dashed\") -> g2\n```\n:::\n\n\n\n\n## The power curve\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng2\n```\n\n::: {.cell-output-display}\n![](power_c33_files/figure-beamer/inference-2-R-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Power curves for means\n\n-   Can also investigate power as it depends on what the true mean is\n    (the farther from null mean 10, the higher the power will be).\n-   Investigate for two different sample sizes, 15 and 30.\n-   First make all combos of mean and sample size:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmeans <- seq(6,10,0.5)\nns <- c(15,30)\ncombos <- crossing(mean=means, n=ns)\n```\n:::\n\n\n\n\n## The combos\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncombos\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 18 x 2\n    mean     n\n   <dbl> <dbl>\n 1   6      15\n 2   6      30\n 3   6.5    15\n 4   6.5    30\n 5   7      15\n 6   7      30\n 7   7.5    15\n 8   7.5    30\n 9   8      15\n10   8      30\n11   8.5    15\n12   8.5    30\n13   9      15\n14   9      30\n15   9.5    15\n16   9.5    30\n17  10      15\n18  10      30\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Calculate powers:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncombos %>% \n  rowwise() %>% \n  mutate(power_stuff = list(power.t.test(n=n, delta=10-mean, sd=4, \n                              type=\"one.sample\"))) %>% \n  mutate(power = power_stuff$power) -> powers\n```\n:::\n\n\n\n\n## then make the plot:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng  <-  ggplot(powers, aes(x = mean, y = power, colour = factor(n))) +\n  geom_point() + geom_line() +\n  geom_hline(yintercept = 1, linetype = \"dashed\") +\n  geom_vline(xintercept = 10, linetype = \"dotted\")\n```\n:::\n\n\n\n\n- Need `n` as categorical so that `colour` works properly.\n\n## The power curves\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](power_c33_files/figure-beamer/inference-2-R-22-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments 1/2\n\n-   When `mean=10`, that is, the true mean equals the null mean, $H_0$\n    is actually true, and the probability of rejecting it then is\n    $\\alpha = 0.05$.\n-   As the null gets more wrong (mean decreases), it becomes easier to\n    correctly reject it.\n-   The blue power curve is above the red one for any mean \\< 10,\n    meaning that no matter how wrong $H_0$ is, you always have a greater\n    chance of correctly rejecting it with a larger sample size.\n\n## Comments 2/2\n\n\n-   Previously, we had $H_0 : \\mu = 10$ and a true $\\mu = 8$, so a mean\n    of 8 produces power 0.42 and 0.80 as shown on the graph.\n-   With $n = 30$, a true mean that is less than about 7 is almost\n    certain to be correctly rejected. (With $n = 15$, the true mean\n    needs to be less than 6.)\n\n## Two-sample power\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n-   For kids learning to read, had sample sizes of 22 (approx) in each\n    group\n-   and these group SDs:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkids %>% group_by(group) %>% \n  summarize(n=n(), s=sd(score))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 3\n  group     n     s\n  <chr> <int> <dbl>\n1 c        23  17.1\n2 t        21  11.0\n```\n\n\n:::\n:::\n\n\n\n\n## Setting up\n\n-   suppose a 5-point improvement in reading score was considered\n    important (on this scale)\n-   in a 2-sample test, null (difference of) mean is zero, so `delta` is\n    true difference in means\n-   what is power for these sample sizes, and what sample size would be\n    needed to get power up to 0.80?\n-   SD in both groups has to be same in `power.t.test`, so take as 14.\n\n## Calculating power for sample size 22 (per group)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(n=22, delta=5, sd=14, type=\"two.sample\", \n             alternative=\"one.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 22\n          delta = 5\n             sd = 14\n      sig.level = 0.05\n          power = 0.3158199\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n\n## sample size for power 0.8\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower.t.test(power=0.80, delta=5, sd=14, type=\"two.sample\", \n             alternative=\"one.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 97.62598\n          delta = 5\n             sd = 14\n      sig.level = 0.05\n          power = 0.8\n    alternative = one.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n\n## Comments\n\n-   The power for the sample sizes we have is very small (to detect a\n    5-point increase).\n-   To get power 0.80, we need 98 kids in *each* group!\n",
    "supporting": [
      "power_c33_files/figure-beamer"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}