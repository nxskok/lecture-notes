{
  "hash": "4560181c47bb998453bda048f7fb8be6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression revisited\"\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n\n## Regression\n\n-   Use regression when one variable is an outcome (*response*, $y$).\n\n-   See if/how response depends on other variable(s), *explanatory*,\n    $x_1, x_2,\\ldots$.\n\n-   Can have *one* or *more than one* explanatory variable, but always\n    one response.\n\n-   Assumes a *straight-line* relationship between response and\n    explanatory.\n\n-   Ask:\n\n    -   *is there* a relationship between $y$ and $x$'s, and if so,\n        which ones?\n    -   what does the relationship look like?\n\n## Packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS, exclude = \"select\") # for Box-Cox, later\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(marginaleffects)\n# library(conflicted) # add these lines if you forget the exclude\n# conflict_prefer(\"select\", \"dplyr\")\n```\n:::\n\n\n\n\n## A regression with one $x$\n\n13 children, measure average total sleep time (ATST, mins) and age\n(years) for each. See if ATST depends on age. Data in `sleep.txt`, ATST\nthen age. Read in data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/sleep.txt\"\nsleep <- read_delim(my_url, \" \")\n```\n:::\n\n\n\n\n## Check data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      atst            age        \n Min.   :461.8   Min.   : 4.400  \n 1st Qu.:491.1   1st Qu.: 7.200  \n Median :528.3   Median : 8.900  \n Mean   :519.3   Mean   : 9.058  \n 3rd Qu.:532.5   3rd Qu.:11.100  \n Max.   :586.0   Max.   :14.000  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 x 2\n    atst   age\n   <dbl> <dbl>\n 1  586   4.4 \n 2  462. 14   \n 3  491. 10.1 \n 4  565   6.7 \n 5  462  11.5 \n 6  532.  9.6 \n 7  478. 12.4 \n 8  515.  8.9 \n 9  493  11.1 \n10  528.  7.75\n11  576.  5.5 \n12  532.  8.6 \n13  530.  7.2 \n```\n\n\n:::\n:::\n\n\n\n\nMake scatter plot of ATST (response) vs. age (explanatory) using code\noverleaf:\n\n## The scatterplot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep, aes(x = age, y = atst)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/suggo-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Correlation\n\n-   Measures how well a straight line fits the data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(sleep, cor(atst, age))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.9515469\n```\n\n\n:::\n:::\n\n\n\n\n-   $1$ is perfect upward trend, $-1$ is perfect downward trend, 0 is no\n    trend.\n\n-   This one close to perfect downward trend.\n\n-   Can do correlations of all pairs of variables:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           atst        age\natst  1.0000000 -0.9515469\nage  -0.9515469  1.0000000\n```\n\n\n:::\n:::\n\n\n\n\n## Lowess curve\n\n-   Sometimes nice to guide the eye: is the trend straight, or not?\n\n-   Idea: *lowess curve*. \"Locally weighted least squares\", not affected\n    by outliers, not constrained to be linear.\n\n-   Lowess is a *guide*: even if straight line appropriate, may\n    wiggle/bend a little. Looking for *serious* problems with linearity.\n\n-   Add lowess curve to plot using `geom_smooth`:\n\n## Plot with lowess curve\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep, aes(x = age, y = atst)) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/icko-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## The regression\n\nScatterplot shows no obvious curve, and a pretty clear downward trend.\nSo we can run the regression:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep.1 <- lm(atst ~ age, data = sleep)\n```\n:::\n\n\n\n\n## The output\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(sleep.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = atst ~ age, data = sleep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.011  -9.365   2.372   6.770  20.411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  646.483     12.918   50.05 2.49e-14 ***\nage          -14.041      1.368  -10.26 5.70e-07 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.15 on 11 degrees of freedom\nMultiple R-squared:  0.9054,\tAdjusted R-squared:  0.8968 \nF-statistic: 105.3 on 1 and 11 DF,  p-value: 5.7e-07\n```\n\n\n:::\n:::\n\n\n\n\n## Conclusions\n\n-   The relationship appears to be a straight line, with a downward\n    trend.\n\n-   $F$-tests for model as a whole and $t$-test for slope (same) both\n    confirm this (P-value $5.7\\times 10^{-7}=0.00000057$).\n\n-   Slope is $-14$, so a 1-year increase in age goes with a 14-minute\n    decrease in ATST on average.\n\n-   R-squared is correlation squared (when one $x$ anyway), between 0\n    and 1 (1 good, 0 bad).\n\n-   Here R-squared is 0.9054, pleasantly high.\n\n## Doing things with the regression output\n\n-   Output from regression (and eg. $t$-test) is all right to look at,\n    but hard to extract and re-use information from.\n\n-   Package `broom` extracts info from model output in way that can be\n    used in pipe (later):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(sleep.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    646.      12.9       50.0 2.49e-14\n2 age            -14.0      1.37     -10.3 5.70e- 7\n```\n\n\n:::\n:::\n\n\n\n\n## also one-line summary of model:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(sleep.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic     p.value    df\n      <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>\n1     0.905         0.897  13.2      105. 0.000000570     1\n# i 6 more variables: logLik <dbl>, AIC <dbl>, BIC <dbl>,\n#   deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n\n## Broom part 2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleep.1 %>% augment(sleep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 x 8\n    atst   age .fitted .resid   .hat .sigma .cooksd\n   <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1  586   4.4     585.   1.30 0.312    13.8 0.00320\n 2  462. 14       450.  11.8  0.341    13.0 0.319  \n 3  491. 10.1     505. -13.6  0.0887   13.0 0.0568 \n 4  565   6.7     552.  12.6  0.137    13.1 0.0844 \n 5  462  11.5     485. -23.0  0.141    11.3 0.294  \n 6  532.  9.6     512.  20.4  0.0801   12.0 0.114  \n 7  478. 12.4     472.   5.23 0.198    13.7 0.0243 \n 8  515.  8.9     522.  -6.32 0.0772   13.6 0.0105 \n 9  493  11.1     491.   2.37 0.122    13.8 0.00258\n10  528.  7.75    538.  -9.37 0.0954   13.4 0.0296 \n11  576.  5.5     569.   6.64 0.214    13.6 0.0441 \n12  532.  8.6     526.   6.77 0.0792   13.6 0.0124 \n13  530.  7.2     545. -14.9  0.114    12.9 0.0933 \n# i 1 more variable: .std.resid <dbl>\n```\n\n\n:::\n:::\n\n\n\n\nUseful for plotting residuals against an $x$-variable.\n\n## CI for mean response and prediction intervals\n\nOnce useful regression exists, use it for prediction:\n\n-   To get a single number for prediction at a given $x$, substitute\n    into regression equation, eg. age 10: predicted ATST is\n    $646.48-14.04(10)=506$ minutes.\n\n-   To express uncertainty of this prediction:\n\n-   *CI for mean response* expresses uncertainty about mean ATST for all\n    children aged 10, based on data.\n\n-   *Prediction interval* expresses uncertainty about predicted ATST for\n    a new child aged 10 whose ATST not known. More uncertain.\n\n-   Also do above for a child aged 5.\n\n## The `marginaleffects` package 1/2\n\nTo get predictions for specific values, set up a dataframe with those\nvalues first:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew <- datagrid(model = sleep.1, age = c(10, 5))\nnew\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age rowid\n1  10     1\n2   5     2\n```\n\n\n:::\n:::\n\n\n\n\nAny variables in the dataframe that you don't specify are set to their\nmean values (quantitative) or most common category (categorical).\n\n## The `marginaleffects` package 2/2\n\nThen feed into `newdata` in `predictions`. This contains a lot of\ncolumns, so you probably want only to display the ones you care about:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(predictions(sleep.1, newdata = new)) %>% \n  select(estimate, conf.low, conf.high, age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  estimate conf.low conf.high age\n1 506.0729 498.4899  513.6558  10\n2 576.2781 563.2588  589.2974   5\n```\n\n\n:::\n:::\n\n\n\n\nThe confidence limits are a 95% confidence interval for the mean\nresponse at that `age`.\n\n## Prediction intervals\n\nThese are obtained (instead) with `predict` as below. Use the same\ndataframe `new` as before:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp <- predict(sleep.1, new, interval = \"p\")\npp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 506.0729 475.8982 536.2475\n2 576.2781 543.8474 608.7088\n```\n\n\n:::\n\n```{.r .cell-code}\ncbind(new, pp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  age rowid      fit      lwr      upr\n1  10     1 506.0729 475.8982 536.2475\n2   5     2 576.2781 543.8474 608.7088\n```\n\n\n:::\n:::\n\n\n\n\n## Plotting the confidence intervals for mean response again:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_predictions(sleep.1, condition = \"age\")\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   Age 10 closer to centre of data, so intervals are both narrower than\n    those for age 5.\n\n-   Prediction intervals bigger than CI for mean (additional\n    uncertainty).\n\n-   Technical note: output from `predict` is R `matrix`, not data frame,\n    so Tidyverse `bind_cols` does not work. Use base R `cbind`.\n\n## That grey envelope\n\nMarks confidence interval for mean for all $x$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep, aes(x = age, y = atst)) + geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_y_continuous(breaks = seq(420, 600, 20))\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Diagnostics\n\nHow to tell whether a straight-line regression is appropriate?\n\n-   Before: check scatterplot for straight trend.\n\n-   After: plot *residuals* (observed minus predicted response) against\n    predicted values. Aim: a plot with no pattern.\n\n## Residual plot\n\nNot much pattern here --- regression appropriate.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleep.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/akjhkadjfhjahnkkk-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## An inappropriate regression\n\nDifferent data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/curvy.txt\"\ncurvy <- read_delim(my_url, \" \")\n```\n:::\n\n\n\n\n## Scatterplot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(curvy, aes(x = xx, y = yy)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-16-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Regression line, anyway\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurvy.1 <- lm(yy ~ xx, data = curvy)\nsummary(curvy.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = yy ~ xx, data = curvy)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.582 -2.204  0.000  1.514  3.509 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   7.5818     1.5616   4.855  0.00126 **\nxx            0.9818     0.2925   3.356  0.00998 **\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.657 on 8 degrees of freedom\nMultiple R-squared:  0.5848,\tAdjusted R-squared:  0.5329 \nF-statistic: 11.27 on 1 and 8 DF,  p-value: 0.009984\n```\n\n\n:::\n:::\n\n\n\n\n## Residual plot\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(curvy.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/altoadige-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## No good: fixing it up\n\n-   Residual plot has *curve*: middle residuals positive, high and low\n    ones negative. Bad.\n\n-   Fitting a curve would be better. Try this:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurvy.2 <- lm(yy ~ xx + I(xx^2), data = curvy)\n```\n:::\n\n\n\n\n-   Adding `xx`-squared term, to allow for curve.\n\n-   Another way to do same thing: specify how model *changes*:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurvy.2a <- update(curvy.1, . ~ . + I(xx^2))\n```\n:::\n\n\n\n\n## Regression 2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(curvy.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    3.9      0.773       5.04 0.00149  \n2 xx             3.74     0.400       9.36 0.0000331\n3 I(xx^2)       -0.307    0.0428     -7.17 0.000182 \n```\n\n\n:::\n\n```{.r .cell-code}\nglance(curvy.2) #\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic   p.value    df\n      <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>\n1     0.950         0.936 0.983      66.8 0.0000275     2\n# i 6 more variables: logLik <dbl>, AIC <dbl>, BIC <dbl>,\n#   deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n\n## Comments\n\n-   `xx`-squared term definitely significant (P-value 0.000182), so need\n    this curve to describe relationship.\n\n-   Adding squared term has made R-squared go up from 0.5848 to 0.9502:\n    great improvement.\n\n-   This is a definite curve!\n\n## The residual plot now\n\nNo problems any more:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(curvy.2, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-21-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Another way to handle curves\n\n-   Above, saw that changing $x$ (adding $x^2$) was a way of handling\n    curved relationships.\n\n-   Another way: change $y$ (transformation).\n\n-   Can guess how to change $y$, or might be theory:\n\n-   example: relationship $y=ae^{bx}$ (exponential growth):\n\n-   take logs to get $\\ln y=\\ln a + bx$.\n\n-   Taking logs has made relationship linear ($\\ln y$ as response).\n\n-   Or, *estimate* transformation, using Box-Cox method.\n\n## Box-Cox\n\n-   Install package `MASS` via `install.packages(\"MASS\")` (only need to\n    do *once*)\n\n-   Every R session you want to use something in `MASS`, type\n    `library(MASS)`\n\n## Some made-up data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/madeup2.csv\"\nmadeup <- read_csv(my_url)\nmadeup\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 x 3\n   ...1     x     y\n  <dbl> <dbl> <dbl>\n1     1     0  17.9\n2     2     1  33.6\n3     3     2  82.7\n4     4     3  31.2\n5     5     4 177. \n6     6     5 359. \n7     7     6 469. \n8     8     7 283. \n```\n\n\n:::\n:::\n\n\n\n\nSeems to be faster-than-linear growth, maybe exponential growth.\n\n## Scatterplot: faster than linear growth\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(madeup, aes(x = x, y = y)) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/dsljhsdjlhf-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Running Box-Cox\n\n-   `library(MASS)` first.\n\n-   Feed `boxcox` a model formula with a squiggle in it, such as you\n    would use for `lm`.\n\n-   Output: a graph (next page):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox(y ~ x, data = madeup)\n```\n:::\n\n\n\n\n## The Box-Cox output\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-beamer/trento-1.pdf)\n:::\n:::\n\n\n\n\n## Comments\n\n-   $\\lambda$ (lambda) is the power by which you should transform $y$ to\n    get the relationship straight (straighter). Power 0 is \"take logs\"\n\n-   Middle dotted line marks best single value of $\\lambda$ (here about\n    0.1).\n\n-   Outer dotted lines mark 95% CI for $\\lambda$, here $-0.3$ to 0.7,\n    approx. (Rather uncertain about best transformation.)\n\n-   Any power transformation within the CI supported by data. In this\n    case, log ($\\lambda=0$) and square root ($\\lambda=0.5$) good, but no\n    transformation ($\\lambda=1$) not.\n\n-   Pick a \"round-number\" value of $\\lambda$ like $2,1,0.5,0,-0.5,-1$.\n    Here 0 and 0.5 good values to pick.\n\n## Did transformation straighten things?\n\n-   Plot transformed $y$ against $x$. Here, log:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(madeup, aes(x = x, y = log(y))) + geom_point() +\n  geom_smooth()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-24-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nLooks much straighter.\n\n## Regression with transformed $y$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmadeup.1 <- lm(log(y) ~ x, data = madeup)\nglance(madeup.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic p.value    df\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>\n1     0.811         0.779 0.588      25.7 0.00228     1\n# i 6 more variables: logLik <dbl>, AIC <dbl>, BIC <dbl>,\n#   deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(madeup.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.03     0.379       7.98 0.000206\n2 x              0.460    0.0907      5.07 0.00228 \n```\n\n\n:::\n:::\n\n\n\n\nR-squared now decently high.\n\n## Multiple regression\n\n-   What if more than one $x$? Extra issues:\n\n    -   Now one intercept and a slope for each $x$: how to interpret?\n\n    -   Which $x$-variables actually help to predict $y$?\n\n    -   Different interpretations of \"global\" $F$-test and individual\n        $t$-tests.\n\n    -   R-squared no longer correlation squared, but still interpreted\n        as \"higher better\".\n\n    -   In `lm` line, add extra $x$s after `~`.\n\n    -   Interpretation not so easy (and other problems that can occur).\n\n## Multiple regression example\n\nStudy of women and visits to health professionals, and how the number of\nvisits might be related to other variables:\n\n\n\n\n```{=tex}\n\\begin{description}\n\\item[timedrs:] number of visits to health professionals (over course of study)\n\\item[phyheal:] number of physical health problems\n\\item[menheal:] number of mental health problems\n\\item[stress:] result of questionnaire about number and type of life changes\n\\end{description}\n```\n\n\n\n`timedrs` response, others explanatory.\n\n## The data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \n  \"http://ritsokiguess.site/datafiles/regressx.txt\"\nvisits <- read_delim(my_url, \" \")\n```\n:::\n\n\n\n\n## Check data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 465 x 5\n   subjno timedrs phyheal menheal stress\n    <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1      1       1       5       8    265\n 2      2       3       4       6    415\n 3      3       0       3       4     92\n 4      4      13       2       2    241\n 5      5      15       3       6     86\n 6      6       3       5       5    247\n 7      7       2       5       6     13\n 8      8       0       4       5     12\n 9      9       7       5       4    269\n10     10       4       3       9    391\n# i 455 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Fit multiple regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisits.1 <- lm(timedrs ~ phyheal + menheal + stress,\n  data = visits)\nsummary(visits.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = timedrs ~ phyheal + menheal + stress, data = visits)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.792  -4.353  -1.815   0.902  65.886 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.704848   1.124195  -3.296 0.001058 ** \nphyheal      1.786948   0.221074   8.083  5.6e-15 ***\nmenheal     -0.009666   0.129029  -0.075 0.940318    \nstress       0.013615   0.003612   3.769 0.000185 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.708 on 461 degrees of freedom\nMultiple R-squared:  0.2188,\tAdjusted R-squared:  0.2137 \nF-statistic: 43.03 on 3 and 461 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## The slopes\n\n-   Model as a whole strongly significant even though R-sq not very big\n    (lots of data). At least one of the $x$'s predicts `timedrs`.\n\n-   The physical health and stress variables definitely help to predict\n    the number of visits, but *with those in the model* we don't need\n    `menheal`. However, look at prediction of `timedrs` from `menheal`\n    by itself:\n\n## Just `menheal`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisits.2 <- lm(timedrs ~ menheal, data = visits)\nsummary(visits.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = timedrs ~ menheal, data = visits)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.826  -5.150  -2.818   1.177  72.513 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.8159     0.8702   4.385 1.44e-05 ***\nmenheal       0.6672     0.1173   5.688 2.28e-08 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.6 on 463 degrees of freedom\nMultiple R-squared:  0.06532,\tAdjusted R-squared:  0.0633 \nF-statistic: 32.35 on 1 and 463 DF,  p-value: 2.279e-08\n```\n\n\n:::\n:::\n\n\n\n\n## `menheal` by itself\n\n-   `menheal` by itself *does* significantly help to predict `timedrs`.\n\n-   But the R-sq is much less (6.5% vs. 22%).\n\n-   So other two variables do a better job of prediction.\n\n-   With those variables in the regression (`phyheal` and `stress`),\n    don't need `menheal` *as well*.\n\n## Investigating via correlation\n\nLeave out first column (`subjno`):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisits %>% select(-subjno) %>% cor()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          timedrs   phyheal   menheal    stress\ntimedrs 1.0000000 0.4395293 0.2555703 0.2865951\nphyheal 0.4395293 1.0000000 0.5049464 0.3055517\nmenheal 0.2555703 0.5049464 1.0000000 0.3697911\nstress  0.2865951 0.3055517 0.3697911 1.0000000\n```\n\n\n:::\n:::\n\n\n\n\n-   `phyheal` most strongly correlated with `timedrs`.\n\n-   Not much to choose between other two.\n\n-   But `menheal` has higher correlation with `phyheal`, so not as much\n    to *add* to prediction as `stress`.\n\n-   Goes to show things more complicated in multiple regression.\n\n## Residual plot (from `timedrs` on all)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(visits.1, aes(x = .fitted, y = .resid)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/iffy8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nApparently random. But...\n\n## Normal quantile plot of residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(visits.1, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-32-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\nNot normal at all; upper tail is way too long.\n\n## Absolute residuals\n\nIs there trend in *size* of residuals (fan-out)? Plot *absolute value*\nof residual against fitted value:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(visits.1, aes(x = .fitted, y = abs(.resid))) +\n  geom_point() + geom_smooth()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   On the normal quantile plot:\n\n    -   highest (most positive) residuals are *way* too high\n\n    -   distribution of residuals skewed to right (not normal at all)\n\n-   On plot of absolute residuals:\n\n    -   size of residuals getting bigger as fitted values increase\n\n    -   predictions getting more variable as fitted values increase\n\n    -   that is, predictions getting *less accurate* as fitted values\n        increase, but predictions should be equally accurate all way\n        along.\n\n-   Both indicate problems with regression, of kind that transformation\n    of response often fixes: that is, predict *function* of response\n    `timedrs` instead of `timedrs` itself.\n\n## Box-Cox transformations\n\n-   Taking log of `timedrs` and having it work: lucky guess. How to find\n    good transformation?\n\n-   Box-Cox again.\n\n-   Extra problem: some of `timedrs` values are 0, but Box-Cox expects\n    all +. Note response for `boxcox`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox(timedrs + 1 ~ phyheal + menheal + stress, data = visits)\n```\n:::\n\n\n\n\n## Try 1\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-36-1.pdf)\n:::\n:::\n\n\n\n\n## Comments on try 1\n\n-   Best: $\\lambda$ just less than zero.\n\n-   Hard to see scale.\n\n-   Focus on $\\lambda$ in $(-0.3,0.1)$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy.lambda <- seq(-0.3, 0.1, 0.01)\nmy.lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] -0.30 -0.29 -0.28 -0.27 -0.26 -0.25 -0.24 -0.23 -0.22\n[10] -0.21 -0.20 -0.19 -0.18 -0.17 -0.16 -0.15 -0.14 -0.13\n[19] -0.12 -0.11 -0.10 -0.09 -0.08 -0.07 -0.06 -0.05 -0.04\n[28] -0.03 -0.02 -0.01  0.00  0.01  0.02  0.03  0.04  0.05\n[37]  0.06  0.07  0.08  0.09  0.10\n```\n\n\n:::\n:::\n\n\n\n\n## Try 2\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox(timedrs + 1 ~ phyheal + menheal + stress,\n  lambda = my.lambda,\n  data = visits\n)\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-38-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   Best: $\\lambda$ just about $-0.07$.\n\n-   CI for $\\lambda$ about $(-0.14,0.01)$.\n\n-   Only nearby round number: $\\lambda=0$, log transformation.\n\n## Fixing the problems\n\n-   Try regression again, with transformed response instead of original\n    one.\n\n-   Then check residual plot to see that it is OK now.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisits.3 <- lm(log(timedrs + 1) ~ phyheal + menheal + stress,\n  data = visits\n)\n```\n:::\n\n\n\n\n-   `timedrs+1` because some `timedrs` values 0, can't take log of 0.\n\n-   Won't usually need to worry about this, but when response could be\n    zero/negative, fix that before transformation.\n\n## Output\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(visits.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log(timedrs + 1) ~ phyheal + menheal + stress, data = visits)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.95865 -0.44076 -0.02331  0.42304  2.36797 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.3903862  0.0882908   4.422 1.22e-05 ***\nphyheal     0.2019361  0.0173624  11.631  < 2e-16 ***\nmenheal     0.0071442  0.0101335   0.705    0.481    \nstress      0.0013158  0.0002837   4.638 4.58e-06 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7625 on 461 degrees of freedom\nMultiple R-squared:  0.3682,\tAdjusted R-squared:  0.3641 \nF-statistic: 89.56 on 3 and 461 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n\n## Comments\n\n-   Model as a whole strongly significant again\n\n-   R-sq higher than before (37% vs. 22%) suggesting things more linear\n    now\n\n-   Same conclusion re `menheal`: can take out of regression.\n\n-   Should look at residual plots (next pages). Have we fixed problems?\n\n## Residuals against fitted values\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(visits.3, aes(x = .fitted, y = .resid)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-41-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Normal quantile plot of residuals\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(visits.3, aes(sample = .resid)) + stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-42-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Absolute residuals against fitted\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(visits.3, aes(x = .fitted, y = abs(.resid))) +\n  geom_point() + geom_smooth()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/bRegression-43-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   Residuals vs. fitted looks a lot more random.\n\n-   Normal quantile plot looks a lot more normal (though still a little\n    right-skewness)\n\n-   Absolute residuals: not so much trend (though still some).\n\n-   Not perfect, but much improved.\n\n## Testing more than one $x$ at once\n\n-   The $t$-tests test only whether one variable could be taken out of\n    the regression you're looking at.\n-   To test significance of more than one variable at once, fit model\n    with and without variables\n    -   then use `anova` to compare fit of models:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvisits.5 <- lm(log(timedrs + 1) ~ phyheal + menheal + stress, \n               data = visits)\nvisits.6 <- lm(log(timedrs + 1) ~ stress, data = visits)\n```\n:::\n\n\n\n\n## Results of tests\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(visits.6, visits.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: log(timedrs + 1) ~ stress\nModel 2: log(timedrs + 1) ~ phyheal + menheal + stress\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    463 371.47                                  \n2    461 268.01  2    103.46 88.984 < 2.2e-16 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n-   Models don't fit equally well, so bigger one fits better.\n\n-   Or \"taking both variables out makes the fit worse, so don't do it\".\n\n-   Taking out those $x$'s is a mistake. Or putting them in is a good\n    idea.\n\n## The punting data\n\nData set `punting.txt` contains 4 variables for 13 right-footed football\nkickers (punters): left leg and right leg strength (lbs), distance\npunted (ft), another variable called \"fred\". Predict punting distance\nfrom other variables:\n\n\\scriptsize\n\n```         \nleft            right               punt         fred\n170               170                162.50       171 \n130               140                144.0        136   \n170               180                174.50       174 \n160               160                163.50       161 \n150               170                192.0        159 \n150               150                171.75       151 \n180               170                162.0        174 \n110               110                104.83       111 \n110               120                105.67       114 \n120               130                117.58       126 \n140               120                140.25       129  \n130               140                150.17       136 \n150               160                165.17       154 \n```\n\n## Reading in\n\n-   Separated by *multiple spaces* with *columns lined up*:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \"http://ritsokiguess.site/datafiles/punting.txt\"\npunting <- read_table(my_url)\n```\n:::\n\n\n\n\n## The data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunting\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 x 4\n    left right  punt  fred\n   <dbl> <dbl> <dbl> <dbl>\n 1   170   170  162.   171\n 2   130   140  144    136\n 3   170   180  174.   174\n 4   160   160  164.   161\n 5   150   170  192    159\n 6   150   150  172.   151\n 7   180   170  162    174\n 8   110   110  105.   111\n 9   110   120  106.   114\n10   120   130  118.   126\n11   140   120  140.   129\n12   130   140  150.   136\n13   150   160  165.   154\n```\n\n\n:::\n:::\n\n\n\n\n## Regression and output\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunting.1 <- lm(punt ~ left + right + fred, data = punting)\nglance(punting.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 x 12\n  r.squared adj.r.squared sigma statistic p.value    df\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>\n1     0.778         0.704  14.7      10.5 0.00267     3\n# i 6 more variables: logLik <dbl>, AIC <dbl>, BIC <dbl>,\n#   deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(punting.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 x 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -4.69      29.1    -0.161    0.876\n2 left           0.268      2.11    0.127    0.902\n3 right          1.05       2.15    0.490    0.636\n4 fred          -0.267      4.23   -0.0632   0.951\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(punting.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = punt ~ left + right + fred, data = punting)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.9325 -11.5618  -0.0315   9.0415  20.0886 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -4.6855    29.1172  -0.161    0.876\nleft          0.2679     2.1111   0.127    0.902\nright         1.0524     2.1477   0.490    0.636\nfred         -0.2672     4.2266  -0.063    0.951\n\nResidual standard error: 14.68 on 9 degrees of freedom\nMultiple R-squared:  0.7781,\tAdjusted R-squared:  0.7042 \nF-statistic: 10.52 on 3 and 9 DF,  p-value: 0.00267\n```\n\n\n:::\n:::\n\n\n\n\n## Comments\n\n-   Overall regression strongly significant, R-sq high.\n\n-   None of the $x$'s significant! Why?\n\n-   $t$-tests only say that you could take any one of the $x$'s out\n    without damaging the fit; doesn't matter which one.\n\n-   Explanation: look at *correlations*.\n\n## The correlations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(punting)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           left     right      punt      fred\nleft  1.0000000 0.8957224 0.8117368 0.9722632\nright 0.8957224 1.0000000 0.8805469 0.9728784\npunt  0.8117368 0.8805469 1.0000000 0.8679507\nfred  0.9722632 0.9728784 0.8679507 1.0000000\n```\n\n\n:::\n:::\n\n\n\n\n-   *All* correlations are high: $x$'s with `punt` (good) and with each\n    other (bad, at least confusing).\n\n-   What to do? Probably do just as well to pick one variable, say\n    `right` since kickers are right-footed.\n\n## Just `right`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunting.2 <- lm(punt ~ right, data = punting)\nsummary(punting.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = punt ~ right, data = punting)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7576 -11.0611   0.3656   7.8890  19.0423 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -3.6930    25.2649  -0.146    0.886    \nright         1.0427     0.1692   6.162 7.09e-05 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.36 on 11 degrees of freedom\nMultiple R-squared:  0.7754,\tAdjusted R-squared:  0.7549 \nF-statistic: 37.97 on 1 and 11 DF,  p-value: 7.088e-05\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(punting.2, punting.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: punt ~ right\nModel 2: punt ~ left + right + fred\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     11 1962.5                           \n2      9 1938.2  2    24.263 0.0563 0.9456\n```\n\n\n:::\n\n```{.r .cell-code}\npunting.3 <- lm(punt ~ left, data = punting)\nsummary(punting.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = punt ~ left, data = punting)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.840 -12.298  -2.234   8.990  35.820 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  12.8834    30.1575   0.427 0.677474    \nleft          0.9553     0.2072   4.610 0.000753 ***\n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.46 on 11 degrees of freedom\nMultiple R-squared:  0.6589,\tAdjusted R-squared:  0.6279 \nF-statistic: 21.25 on 1 and 11 DF,  p-value: 0.0007528\n```\n\n\n:::\n:::\n\n\n\n\nNo significant loss by dropping other two variables.\n\n## Comparing R-squareds\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(punting.1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7781401\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(punting.2)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7753629\n```\n\n\n:::\n:::\n\n\n\n\nBasically no difference. In regression (over), `right` significant:\n\n## Regression results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(punting.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)    -3.69    25.3      -0.146 0.886    \n2 right           1.04     0.169     6.16  0.0000709\n```\n\n\n:::\n:::\n\n\n\n\n## But\\ldots\n\n-   Maybe we got the *form* of the relationship with `left` wrong.\n\n-   Check: plot *residuals* from previous regression (without `left`)\n    against `left`.\n\n-   Residuals here are \"punting distance adjusted for right leg\n    strength\".\n\n-   If there is some kind of relationship with `left`, we should include\n    in model.\n\n-   Plot of residuals against original variable: `augment` from `broom`.\n\n## Augmenting `punting.2`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunting.2 %>% augment(punting) -> punting.2.aug\npunting.2.aug \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 x 10\n    left right  punt  fred .fitted  .resid   .hat .sigma\n   <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl>  <dbl>  <dbl>\n 1   170   170  162.   171    174. -11.1   0.157    13.5\n 2   130   140  144    136    142.   1.72  0.0864   14.0\n 3   170   180  174.   174    184.  -9.49  0.244    13.6\n 4   160   160  164.   161    163.   0.366 0.101    14.0\n 5   150   170  192    159    174.  18.4   0.157    12.5\n 6   150   150  172.   151    153.  19.0   0.0778   12.5\n 7   180   170  162    174    174. -11.6   0.157    13.4\n 8   110   110  105.   111    111.  -6.17  0.305    13.8\n 9   110   120  106.   114    121. -15.8   0.2      12.9\n10   120   130  118.   126    132. -14.3   0.127    13.1\n11   140   120  140.   129    121.  18.8   0.2      12.3\n12   130   140  150.   136    142.   7.89  0.0864   13.8\n13   150   160  165.   154    163.   2.04  0.101    14.0\n# i 2 more variables: .cooksd <dbl>, .std.resid <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n## Residuals against `left`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(punting.2.aug, aes(x = left, y = .resid)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](regression_files/figure-beamer/basingstoke-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Comments\n\n-   There is a *curved* relationship with `left`.\n\n-   We should add `left`-squared to the regression (and therefore put\n    `left` back in when we do that):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npunting.3 <- lm(punt ~ left + I(left^2) + right,\n  data = punting\n)\n```\n:::\n\n\n\n\n## Regression with `left-squared`\n\n\\scriptsize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(punting.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = punt ~ left + I(left^2) + right, data = punting)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.3777  -5.3599   0.0459   4.5088  13.2669 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -4.623e+02  9.902e+01  -4.669  0.00117 **\nleft         6.888e+00  1.462e+00   4.710  0.00110 **\nI(left^2)   -2.302e-02  4.927e-03  -4.672  0.00117 **\nright        7.396e-01  2.292e-01   3.227  0.01038 * \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.931 on 9 degrees of freedom\nMultiple R-squared:  0.9352,\tAdjusted R-squared:  0.9136 \nF-statistic:  43.3 on 3 and 9 DF,  p-value: 1.13e-05\n```\n\n\n:::\n:::\n\n\n\n\n## Comments\n\n-   This was definitely a good idea (R-squared has clearly increased).\n\n-   We would never have seen it without plotting residuals from\n    `punting.2` (without `left`) against `left`.\n\n-   Negative slope for `leftsq` means that increased left-leg strength\n    only increases punting distance up to a point: beyond that, it\n    decreases again.\n",
    "supporting": [
      "regression_files/figure-beamer"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}