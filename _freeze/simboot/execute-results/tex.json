{
  "hash": "9d9f86b4229618b0e36253703d83e97b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulation and the bootstrap\"\nformat: \n  revealjs:\n     df-print: paged\n     scrollable: true\n     embed-resources: true\n  beamer:\n    incremental: false\nexecute: \n  echo: true\n---\n\n\n\n\n## packages\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n\n## Simulation\n\n- Sometimes you know the exact mathematical answer to a problem, eg:\n  - $X_1,  \\ldots X_n \\sim N(\\mu, \\sigma^2)$, what is distribution of $\\bar{X}$? (Ans: $N(\\mu, \\sigma^2/n)$.)\n- More often, though, you don't:\n  - if $X \\sim Bin(2, 0.5), Y \\sim Bin(3, 0.2)$, what is dist of $Z = X+Y$?\n  \n- Simulation: generate random $X$ and $Y$, calculate sum, repeat many times. Gives you (approx) dist of $X+Y$ without any mathematics!\n\n## Random numbers in R\n\n- R knows about a lot of distributions, eg: `norm binom pois exp gamma t chisq` (type `?distributions` in Console to see more)\n- to generate random numbers from a distribution, put `r` on front of these; inputs are number of random values to generate, and parameters of distribution to simulate from.\n- Examples:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(5, 100, 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 111.96072  89.80331 113.74231  90.74167  89.86461\n```\n\n\n:::\n:::\n\n\n\n\nand \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrpois(10, 3.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 5 2 5 5 3 5 1 3 4 0\n```\n\n\n:::\n:::\n\n\n\n\n## Our problem: simulating once\n\n- if $X \\sim Bin(2, 0.5), Y \\sim Bin(3, 0.2)$, what is dist of $Z = X+Y$?\n \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rbinom(1, 2, 0.5)\ny <- rbinom(1, 3, 0.2)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\ny\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\nx + y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n\n\nTo simulate many times: \n- set up dataframe with space for each simulated value\n- work rowwise\n- do one simulation per row\n\n## Simulating many times\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(x = rbinom(1, 2, 0.5),\n         y = rbinom(1, 3, 0.2),\n         z = x + y) -> d\n```\n:::\n\n\n\n\n## Results\n\n\\small \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 x 4\n# Rowwise: \n     sim     x     y     z\n   <int> <int> <int> <int>\n 1     1     1     0     1\n 2     2     2     1     3\n 3     3     1     0     1\n 4     4     0     0     0\n 5     5     1     0     1\n 6     6     2     0     2\n 7     7     1     0     1\n 8     8     0     0     0\n 9     9     0     2     2\n10    10     0     1     1\n# i 9,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Distribution of sum\n\nMake a bar chart rather than a histogram because distribution of $Z$ is discrete:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = z)) + geom_bar()\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## (Simulated) probability that the sum is at least 4:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% count(z >= 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 x 2\n# Rowwise: \n  `z >= 4`     n\n  <lgl>    <int>\n1 FALSE     9715\n2 TRUE       285\n```\n\n\n:::\n:::\n\n\n\n\nOnly this much:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n285/10000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0285\n```\n\n\n:::\n:::\n\n\n\n\nA sum of 5 is possible though very unlikely.\n\n## The bootstrap\n \nSource: [Hesterberg et al](https://drive.google.com/file/d/10s780DfNtfSs1YqFzrUkq1mcHAEw6WNF/edit)\n\n- Sampling distribution of a statistic is distribution of that statistic over \"all possible samples\" from population of interest.\n- \"Plug-in principle\": sample mean estimates population mean, sample variance estimates population variance, etc.\n- Also, sample is estimate of population (precisely, proportion of sample values $\\le x$ estimates probability of drawing value $\\le x$ from population, for any $x$).\n- As long as your sample is representative, sampling *from the sample* (!) is an estimate of sampling from the population. Called a *bootstrap sample*.\n- Sample from sample *with* replacement, or else you get original sample back.\n\n## Blue Jays attendances:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(jays, aes(sample = attendance)) + \n  stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/inference-1-R-66-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- $t$ procedure for the mean may not be a good idea because the distribution is skewed.\n- Previously: hand-waving with sample size.\n\n## What *actually* matters\n\n- It's not the distribution of the *data* that has to be approx normal (for a $t$ procedure).\n- What matters is the *sampling distribution of the sample mean*.\n- If the sample size is large enough, the sampling distribution will be normal enough even if the data distribution is not.\n  - This is why we had to consider the sample size as well as the shape.\n- But how do we know whether this is the case or not? We only have *one* sample.\n- Use the bootstrap to simulate sampling distribution.\n\n## Simulating the sampling distribution of sample statistic\n\n- Sample from our sample *with replacement*.\n- Calculate statistic\n- Repeat many times (simulation).\n- This gives an idea of how our statistic might vary in repeated samples: that is, its sampling distribution.\n- Called the **bootstrap distribution** of the statistic.\n- If the bootstrap distribution is approx normal, infer that the true sampling distribution also approx normal, therefore inference about the mean such as $t$ is good enough.\n- If not, we should be more careful.\n\n\n## Bootstrapping the Blue Jays attendances\n\n- Sampling with replacement is done like this (the default sample size is as long as the original data):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot <- sample(jays$attendance, replace=TRUE)\nmean(boot)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23764.76\n```\n\n\n:::\n:::\n\n\n\n\n- That's one bootstrapped mean. We need a whole bunch.\n\n## Comparing the actual sample with the bootstrapped one\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\\footnotesize\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(jays$attendance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 14184 14433 15062 15086 15168 15606 16402 17264 17276 18581\n[11] 19014 19217 21195 21312 21397 21519 29306 30430 33086 34743\n[21] 37929 42419 42917 44794 48414\n```\n\n\n:::\n\n```{.r .cell-code}\nsort(boot)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 14184 14433 14433 15086 15086 15168 15168 16402 17264 17264\n[11] 17276 17276 17276 17276 18581 19014 21312 21519 33086 37929\n[21] 42419 42419 42917 42917 48414\n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\nBootstrap sample has repeats plus missing values from original sample.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n## A whole bunch\n\n- We are now doing a simulation. I like 10,000 samples when testing for normality:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(boot_sample = list(sample(jays$attendance, replace = TRUE))) %>% \n  mutate(my_mean = mean(boot_sample)) -> samples\n```\n:::\n\n\n\n\n- for each row:\n  - obtain a bootstrap sample (`list` because we are saving the whole sample in one cell of the dataframe)\n  - work out the mean of that bootstrap sample.\n  \n## Bootstrap sample means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 x 3\n# Rowwise: \n     sim boot_sample my_mean\n   <int> <list>        <dbl>\n 1     1 <dbl [25]>   23055.\n 2     2 <dbl [25]>   25513.\n 3     3 <dbl [25]>   25563.\n 4     4 <dbl [25]>   29198.\n 5     5 <dbl [25]>   23615.\n 6     6 <dbl [25]>   28472.\n 7     7 <dbl [25]>   28648.\n 8     8 <dbl [25]>   23329.\n 9     9 <dbl [25]>   24808.\n10    10 <dbl [25]>   24665.\n# i 9,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n  \n## Sampling distribution of sample mean\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(samples, aes(x=my_mean)) + geom_histogram(bins=10)\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/bootstrap-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- Is that a slightly long right tail?\n\n## Normal quantile plot\n\nmight be better than a histogram:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(samples, aes(sample = my_mean)) + \n  stat_qq()+stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/bootstrap-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- a very very slight right-skewness, but very close to normal.\n- hence the $t$-test is fine for the Blue Jays attendances.\n\n## Kids learning to read\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n- Normal quantile plots, one for each sample:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(kids, aes(sample = score)) + \n  stat_qq() + stat_qq_line() +\n  facet_wrap(~ group)\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/inference-1-R-14-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- These both look close to normal.\n\n## Control group\n\n- Pull out control group children\n- Obtain bootstrap sampling distribution of scores\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkids %>% filter(group == \"c\") -> controls\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(controls$score, \n                                 replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) -> samples1\n```\n:::\n\n\n\n\n## Bootstrap sample means\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 x 3\n# Rowwise: \n     sim my_sample  my_mean\n   <int> <list>       <dbl>\n 1     1 <dbl [23]>    43.7\n 2     2 <dbl [23]>    38.9\n 3     3 <dbl [23]>    44.3\n 4     4 <dbl [23]>    40.8\n 5     5 <dbl [23]>    42.9\n 6     6 <dbl [23]>    37.3\n 7     7 <dbl [23]>    42.2\n 8     8 <dbl [23]>    46.7\n 9     9 <dbl [23]>    40.5\n10    10 <dbl [23]>    39.2\n# i 9,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## The bootstrap sampling distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(samples1, aes(sample = my_mean)) +\n  stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/unnamed-chunk-17-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- Very close to normal.\n\n## Same, for treatment group\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkids %>% filter(group == \"t\") -> treated\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(treated$score, \n                                 replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) -> samples2\n```\n:::\n\n\n\n\n## The bootstrap sampling distribution\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(samples2, aes(sample = my_mean)) +\n  stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- Very slightly left-skewed, but close to normal. Not a problem.\n\n\n## Pain relief\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n- With matched pairs, assumption is of normality of *differences*:\n\n\\small\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npain %>% mutate(diff = druga - drugb) -> pain\npain\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 12 x 4\n   subject druga drugb    diff\n     <dbl> <dbl> <dbl>   <dbl>\n 1       1   2     3.5  -1.5  \n 2       2   3.6   5.7  -2.1  \n 3       3   2.6   2.9  -0.300\n 4       4   2.6   2.4   0.200\n 5       5   7.3   9.9  -2.6  \n 6       6   3.4   3.3   0.100\n 7       7  14.9  16.7  -1.80 \n 8       8   6.6   6     0.600\n 9       9   2.3   3.8  -1.5  \n10      10   2     4    -2    \n11      11   6.8   9.1  -2.3  \n12      12   8.5  20.9 -12.4  \n```\n\n\n:::\n:::\n\n\n\n\n\\normalsize\n\n## Bootstrap sampling distribution of differences\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(sim = 1:10000) %>% \n  rowwise() %>% \n  mutate(my_sample = list(sample(pain$diff, replace = TRUE))) %>% \n  mutate(my_mean = mean(my_sample)) -> samples\n```\n:::\n\n\n\n\n## Result\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,000 x 3\n# Rowwise: \n     sim my_sample  my_mean\n   <int> <list>       <dbl>\n 1     1 <dbl [12]>  -1.82 \n 2     2 <dbl [12]>  -3.58 \n 3     3 <dbl [12]>  -2.66 \n 4     4 <dbl [12]>  -2.9  \n 5     5 <dbl [12]>  -0.975\n 6     6 <dbl [12]>  -2.25 \n 7     7 <dbl [12]>  -2.32 \n 8     8 <dbl [12]>  -1.84 \n 9     9 <dbl [12]>  -4.77 \n10    10 <dbl [12]>  -2.42 \n# i 9,990 more rows\n```\n\n\n:::\n:::\n\n\n\n\n\n## Assess it for normality\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(samples, aes(sample = my_mean)) +\n  stat_qq() + stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/unnamed-chunk-23-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- Very skewed to the left (because of low outlier)\n- Matched pairs $t$ not to be trusted at all.\n\n## Histogram with many bins\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(samples, aes(x = my_mean)) + geom_histogram(bins = 60)\n```\n\n::: {.cell-output-display}\n![](simboot_files/figure-beamer/unnamed-chunk-24-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n- actually a very multimodal distribution: one mode for each time the low outlier appears in the bootstrap sampling distribution (can be none at all up to several times).",
    "supporting": [
      "simboot_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}